{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 24401.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  24.087810516357422\n",
      "tt = 1 : 46 % \n",
      "Y_0 = 1 : 47 % \n",
      "Y_1 = 1 : 63 % \n",
      "KLD = 0.10335976631805187\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6921,\ttrain_loss_classif: 0.6916,\ttrain_loss_wass: 0.0530,\tval_loss: 0.6943,\tval_loss_classif: 0.6939,\tval_loss_wass: 0.0451\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.6880,\ttrain_loss_classif: 0.6876,\ttrain_loss_wass: 0.0400,\tval_loss: 0.6970,\tval_loss_classif: 0.6966,\tval_loss_wass: 0.0426\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.6820,\ttrain_loss_classif: 0.6817,\ttrain_loss_wass: 0.0354,\tval_loss: 0.6980,\tval_loss_classif: 0.6976,\tval_loss_wass: 0.0406\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.6793,\ttrain_loss_classif: 0.6790,\ttrain_loss_wass: 0.0366,\tval_loss: 0.6991,\tval_loss_classif: 0.6988,\tval_loss_wass: 0.0384\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.6731,\ttrain_loss_classif: 0.6727,\ttrain_loss_wass: 0.0326,\tval_loss: 0.6888,\tval_loss_classif: 0.6884,\tval_loss_wass: 0.0345\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.6642,\ttrain_loss_classif: 0.6639,\ttrain_loss_wass: 0.0290,\tval_loss: 0.6695,\tval_loss_classif: 0.6691,\tval_loss_wass: 0.0336\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.6507,\ttrain_loss_classif: 0.6504,\ttrain_loss_wass: 0.0315,\tval_loss: 0.6445,\tval_loss_classif: 0.6441,\tval_loss_wass: 0.0373\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.6289,\ttrain_loss_classif: 0.6286,\ttrain_loss_wass: 0.0324,\tval_loss: 0.6116,\tval_loss_classif: 0.6112,\tval_loss_wass: 0.0394\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 0.5817,\ttrain_loss_classif: 0.5814,\ttrain_loss_wass: 0.0357,\tval_loss: 0.5499,\tval_loss_classif: 0.5495,\tval_loss_wass: 0.0452\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.5315,\ttrain_loss_classif: 0.5310,\ttrain_loss_wass: 0.0492,\tval_loss: 0.4797,\tval_loss_classif: 0.4792,\tval_loss_wass: 0.0530\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 0.4663,\ttrain_loss_classif: 0.4658,\ttrain_loss_wass: 0.0528,\tval_loss: 0.4072,\tval_loss_classif: 0.4066,\tval_loss_wass: 0.0611\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 0.4242,\ttrain_loss_classif: 0.4236,\ttrain_loss_wass: 0.0639,\tval_loss: 0.3696,\tval_loss_classif: 0.3690,\tval_loss_wass: 0.0669\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 0.3987,\ttrain_loss_classif: 0.3981,\ttrain_loss_wass: 0.0625,\tval_loss: 0.3378,\tval_loss_classif: 0.3371,\tval_loss_wass: 0.0668\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 0.3714,\ttrain_loss_classif: 0.3708,\ttrain_loss_wass: 0.0584,\tval_loss: 0.3425,\tval_loss_classif: 0.3418,\tval_loss_wass: 0.0625\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 0.3511,\ttrain_loss_classif: 0.3506,\ttrain_loss_wass: 0.0501,\tval_loss: 0.3478,\tval_loss_classif: 0.3472,\tval_loss_wass: 0.0647\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 0.3312,\ttrain_loss_classif: 0.3306,\ttrain_loss_wass: 0.0573,\tval_loss: 0.3529,\tval_loss_classif: 0.3522,\tval_loss_wass: 0.0623\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 0.3152,\ttrain_loss_classif: 0.3146,\ttrain_loss_wass: 0.0557,\tval_loss: 0.3813,\tval_loss_classif: 0.3806,\tval_loss_wass: 0.0631\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 0.2937,\ttrain_loss_classif: 0.2932,\ttrain_loss_wass: 0.0566,\tval_loss: 0.3823,\tval_loss_classif: 0.3816,\tval_loss_wass: 0.0672\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 0.2862,\ttrain_loss_classif: 0.2856,\ttrain_loss_wass: 0.0635,\tval_loss: 0.4171,\tval_loss_classif: 0.4165,\tval_loss_wass: 0.0687\n",
      "19:\t[0s / 2s],\t\ttrain_loss: 0.2753,\ttrain_loss_classif: 0.2746,\ttrain_loss_wass: 0.0616,\tval_loss: 0.4000,\tval_loss_classif: 0.3993,\tval_loss_wass: 0.0689\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:01:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.766  0.778  0.851  0.886  0.117  0.139  0.133       0.178   \n",
      "lgbm       0.812  0.838  0.798  0.818  0.136  0.178  0.130       0.247   \n",
      "xgb        0.812  0.838  0.811  0.823  0.130  0.166  0.129       0.247   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.193  \n",
      "lgbm            0.237  \n",
      "xgb             0.240  \n",
      "{'wd_param': 0.0, 'wd': 24.087810516357422, 'y_0_perc': 47, 'y_1_perc': 63}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_0.0_wd_24.087810516357422}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.766 &  0.778 &  0.851 &  0.886 & 0.117 &  0.139 &  0.133 &       0.178 &       0.193 \\\\\n",
      " 0.812 &  0.838 &  0.798 &  0.818 & 0.136 &  0.178 &  0.130 &       0.247 &       0.237 \\\\\n",
      " 0.812 &  0.838 &  0.811 &  0.823 & 0.130 &  0.166 &  0.129 &       0.247 &       0.240 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:15<00:30, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  24.286033630371094\n",
      "tt = 1 : 47 % \n",
      "Y_0 = 1 : 47 % \n",
      "Y_1 = 1 : 62 % \n",
      "KLD = 0.10521181399301974\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6910,\ttrain_loss_classif: 0.6907,\ttrain_loss_wass: 0.0300,\tval_loss: 0.6923,\tval_loss_classif: 0.6917,\tval_loss_wass: 0.0544\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6884,\ttrain_loss_classif: 0.6880,\ttrain_loss_wass: 0.0357,\tval_loss: 0.6918,\tval_loss_classif: 0.6912,\tval_loss_wass: 0.0533\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6837,\ttrain_loss_classif: 0.6835,\ttrain_loss_wass: 0.0223,\tval_loss: 0.6911,\tval_loss_classif: 0.6906,\tval_loss_wass: 0.0507\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6846,\ttrain_loss_classif: 0.6844,\ttrain_loss_wass: 0.0223,\tval_loss: 0.6913,\tval_loss_classif: 0.6908,\tval_loss_wass: 0.0489\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 0.6800,\ttrain_loss_classif: 0.6798,\ttrain_loss_wass: 0.0189,\tval_loss: 0.6880,\tval_loss_classif: 0.6875,\tval_loss_wass: 0.0454\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 0.6782,\ttrain_loss_classif: 0.6780,\ttrain_loss_wass: 0.0255,\tval_loss: 0.6829,\tval_loss_classif: 0.6825,\tval_loss_wass: 0.0444\n",
      "6:\t[0s / 3s],\t\ttrain_loss: 0.6742,\ttrain_loss_classif: 0.6740,\ttrain_loss_wass: 0.0167,\tval_loss: 0.6750,\tval_loss_classif: 0.6746,\tval_loss_wass: 0.0462\n",
      "7:\t[0s / 4s],\t\ttrain_loss: 0.6615,\ttrain_loss_classif: 0.6613,\ttrain_loss_wass: 0.0206,\tval_loss: 0.6594,\tval_loss_classif: 0.6588,\tval_loss_wass: 0.0522\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 0.6386,\ttrain_loss_classif: 0.6383,\ttrain_loss_wass: 0.0270,\tval_loss: 0.6324,\tval_loss_classif: 0.6318,\tval_loss_wass: 0.0600\n",
      "9:\t[0s / 5s],\t\ttrain_loss: 0.5912,\ttrain_loss_classif: 0.5908,\ttrain_loss_wass: 0.0322,\tval_loss: 0.5847,\tval_loss_classif: 0.5840,\tval_loss_wass: 0.0695\n",
      "10:\t[0s / 6s],\t\ttrain_loss: 0.5235,\ttrain_loss_classif: 0.5230,\ttrain_loss_wass: 0.0439,\tval_loss: 0.5507,\tval_loss_classif: 0.5498,\tval_loss_wass: 0.0844\n",
      "11:\t[0s / 6s],\t\ttrain_loss: 0.4611,\ttrain_loss_classif: 0.4606,\ttrain_loss_wass: 0.0495,\tval_loss: 0.5566,\tval_loss_classif: 0.5555,\tval_loss_wass: 0.1062\n",
      "12:\t[0s / 7s],\t\ttrain_loss: 0.4173,\ttrain_loss_classif: 0.4165,\ttrain_loss_wass: 0.0786,\tval_loss: 0.5774,\tval_loss_classif: 0.5761,\tval_loss_wass: 0.1224\n",
      "13:\t[0s / 8s],\t\ttrain_loss: 0.3902,\ttrain_loss_classif: 0.3897,\ttrain_loss_wass: 0.0532,\tval_loss: 0.5629,\tval_loss_classif: 0.5618,\tval_loss_wass: 0.1101\n",
      "14:\t[0s / 8s],\t\ttrain_loss: 0.3672,\ttrain_loss_classif: 0.3667,\ttrain_loss_wass: 0.0526,\tval_loss: 0.5137,\tval_loss_classif: 0.5127,\tval_loss_wass: 0.1034\n",
      "15:\t[0s / 9s],\t\ttrain_loss: 0.3476,\ttrain_loss_classif: 0.3470,\ttrain_loss_wass: 0.0514,\tval_loss: 0.4802,\tval_loss_classif: 0.4792,\tval_loss_wass: 0.0934\n",
      "16:\t[0s / 9s],\t\ttrain_loss: 0.3372,\ttrain_loss_classif: 0.3367,\ttrain_loss_wass: 0.0508,\tval_loss: 0.4631,\tval_loss_classif: 0.4622,\tval_loss_wass: 0.0908\n",
      "17:\t[0s / 10s],\t\ttrain_loss: 0.3212,\ttrain_loss_classif: 0.3207,\ttrain_loss_wass: 0.0485,\tval_loss: 0.4741,\tval_loss_classif: 0.4732,\tval_loss_wass: 0.0885\n",
      "18:\t[0s / 11s],\t\ttrain_loss: 0.3008,\ttrain_loss_classif: 0.3003,\ttrain_loss_wass: 0.0560,\tval_loss: 0.4596,\tval_loss_classif: 0.4588,\tval_loss_wass: 0.0863\n",
      "19:\t[0s / 11s],\t\ttrain_loss: 0.2968,\ttrain_loss_classif: 0.2962,\ttrain_loss_wass: 0.0595,\tval_loss: 0.4844,\tval_loss_classif: 0.4836,\tval_loss_wass: 0.0844\n",
      "20:\t[0s / 12s],\t\ttrain_loss: 0.2795,\ttrain_loss_classif: 0.2791,\ttrain_loss_wass: 0.0460,\tval_loss: 0.4875,\tval_loss_classif: 0.4867,\tval_loss_wass: 0.0837\n",
      "21:\t[0s / 12s],\t\ttrain_loss: 0.2699,\ttrain_loss_classif: 0.2695,\ttrain_loss_wass: 0.0387,\tval_loss: 0.4853,\tval_loss_classif: 0.4845,\tval_loss_wass: 0.0817\n",
      "22:\t[0s / 13s],\t\ttrain_loss: 0.2562,\ttrain_loss_classif: 0.2558,\ttrain_loss_wass: 0.0406,\tval_loss: 0.4800,\tval_loss_classif: 0.4792,\tval_loss_wass: 0.0807\n",
      "23:\t[0s / 14s],\t\ttrain_loss: 0.2461,\ttrain_loss_classif: 0.2457,\ttrain_loss_wass: 0.0435,\tval_loss: 0.4931,\tval_loss_classif: 0.4923,\tval_loss_wass: 0.0826\n",
      "24:\t[0s / 14s],\t\ttrain_loss: 0.2360,\ttrain_loss_classif: 0.2355,\ttrain_loss_wass: 0.0506,\tval_loss: 0.5062,\tval_loss_classif: 0.5053,\tval_loss_wass: 0.0828\n",
      "25:\t[0s / 15s],\t\ttrain_loss: 0.2307,\ttrain_loss_classif: 0.2301,\ttrain_loss_wass: 0.0534,\tval_loss: 0.5227,\tval_loss_classif: 0.5219,\tval_loss_wass: 0.0853\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:02:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.819  0.800  0.911  0.888  0.095  0.105  0.076       0.159   \n",
      "lgbm       0.862  0.828  0.848  0.845  0.134  0.176  0.145       0.243   \n",
      "xgb        0.862  0.828  0.856  0.837  0.131  0.176  0.159       0.250   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.145  \n",
      "lgbm            0.233  \n",
      "xgb             0.244  \n",
      "{'wd_param': 0.0, 'wd': 24.286033630371094, 'y_0_perc': 47, 'y_1_perc': 62}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_0.0_wd_24.286033630371094}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.819 &  0.800 &  0.911 &  0.888 & 0.095 &  0.105 &  0.076 &       0.159 &       0.145 \\\\\n",
      " 0.862 &  0.828 &  0.848 &  0.845 & 0.134 &  0.176 &  0.145 &       0.243 &       0.233 \\\\\n",
      " 0.862 &  0.828 &  0.856 &  0.837 & 0.131 &  0.176 &  0.159 &       0.250 &       0.244 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:09<00:38, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  24.866914749145508\n",
      "tt = 1 : 48 % \n",
      "Y_0 = 1 : 48 % \n",
      "Y_1 = 1 : 65 % \n",
      "KLD = 0.102603015035136\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6941,\ttrain_loss_classif: 0.6937,\ttrain_loss_wass: 0.0352,\tval_loss: 0.6914,\tval_loss_classif: 0.6907,\tval_loss_wass: 0.0629\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6846,\ttrain_loss_classif: 0.6842,\ttrain_loss_wass: 0.0365,\tval_loss: 0.6887,\tval_loss_classif: 0.6881,\tval_loss_wass: 0.0560\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6765,\ttrain_loss_classif: 0.6761,\ttrain_loss_wass: 0.0374,\tval_loss: 0.6872,\tval_loss_classif: 0.6866,\tval_loss_wass: 0.0541\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6666,\ttrain_loss_classif: 0.6662,\ttrain_loss_wass: 0.0340,\tval_loss: 0.6875,\tval_loss_classif: 0.6870,\tval_loss_wass: 0.0555\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 0.6652,\ttrain_loss_classif: 0.6648,\ttrain_loss_wass: 0.0410,\tval_loss: 0.6866,\tval_loss_classif: 0.6861,\tval_loss_wass: 0.0560\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 0.6483,\ttrain_loss_classif: 0.6479,\ttrain_loss_wass: 0.0393,\tval_loss: 0.6689,\tval_loss_classif: 0.6683,\tval_loss_wass: 0.0585\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 0.6307,\ttrain_loss_classif: 0.6303,\ttrain_loss_wass: 0.0417,\tval_loss: 0.6443,\tval_loss_classif: 0.6435,\tval_loss_wass: 0.0782\n",
      "7:\t[0s / 4s],\t\ttrain_loss: 0.5941,\ttrain_loss_classif: 0.5936,\ttrain_loss_wass: 0.0500,\tval_loss: 0.6070,\tval_loss_classif: 0.6060,\tval_loss_wass: 0.1004\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 0.5495,\ttrain_loss_classif: 0.5490,\ttrain_loss_wass: 0.0530,\tval_loss: 0.5550,\tval_loss_classif: 0.5539,\tval_loss_wass: 0.1158\n",
      "9:\t[0s / 6s],\t\ttrain_loss: 0.4993,\ttrain_loss_classif: 0.4985,\ttrain_loss_wass: 0.0834,\tval_loss: 0.5026,\tval_loss_classif: 0.5011,\tval_loss_wass: 0.1430\n",
      "10:\t[0s / 6s],\t\ttrain_loss: 0.4667,\ttrain_loss_classif: 0.4660,\ttrain_loss_wass: 0.0643,\tval_loss: 0.4504,\tval_loss_classif: 0.4487,\tval_loss_wass: 0.1734\n",
      "11:\t[0s / 7s],\t\ttrain_loss: 0.4435,\ttrain_loss_classif: 0.4426,\ttrain_loss_wass: 0.0869,\tval_loss: 0.4123,\tval_loss_classif: 0.4103,\tval_loss_wass: 0.1988\n",
      "12:\t[0s / 7s],\t\ttrain_loss: 0.4061,\ttrain_loss_classif: 0.4051,\ttrain_loss_wass: 0.0931,\tval_loss: 0.3967,\tval_loss_classif: 0.3947,\tval_loss_wass: 0.2036\n",
      "13:\t[0s / 8s],\t\ttrain_loss: 0.3858,\ttrain_loss_classif: 0.3847,\ttrain_loss_wass: 0.1039,\tval_loss: 0.4321,\tval_loss_classif: 0.4301,\tval_loss_wass: 0.2026\n",
      "14:\t[0s / 9s],\t\ttrain_loss: 0.3372,\ttrain_loss_classif: 0.3362,\ttrain_loss_wass: 0.1042,\tval_loss: 0.3856,\tval_loss_classif: 0.3837,\tval_loss_wass: 0.1912\n",
      "15:\t[0s / 9s],\t\ttrain_loss: 0.3297,\ttrain_loss_classif: 0.3288,\ttrain_loss_wass: 0.0929,\tval_loss: 0.3978,\tval_loss_classif: 0.3960,\tval_loss_wass: 0.1798\n",
      "16:\t[0s / 10s],\t\ttrain_loss: 0.3002,\ttrain_loss_classif: 0.2992,\ttrain_loss_wass: 0.0915,\tval_loss: 0.3963,\tval_loss_classif: 0.3946,\tval_loss_wass: 0.1707\n",
      "17:\t[0s / 10s],\t\ttrain_loss: 0.2776,\ttrain_loss_classif: 0.2766,\ttrain_loss_wass: 0.0928,\tval_loss: 0.4004,\tval_loss_classif: 0.3987,\tval_loss_wass: 0.1670\n",
      "18:\t[0s / 11s],\t\ttrain_loss: 0.2548,\ttrain_loss_classif: 0.2540,\ttrain_loss_wass: 0.0819,\tval_loss: 0.4226,\tval_loss_classif: 0.4210,\tval_loss_wass: 0.1673\n",
      "19:\t[0s / 11s],\t\ttrain_loss: 0.2479,\ttrain_loss_classif: 0.2469,\ttrain_loss_wass: 0.0949,\tval_loss: 0.4356,\tval_loss_classif: 0.4339,\tval_loss_wass: 0.1663\n",
      "20:\t[0s / 12s],\t\ttrain_loss: 0.2187,\ttrain_loss_classif: 0.2177,\ttrain_loss_wass: 0.1038,\tval_loss: 0.4506,\tval_loss_classif: 0.4490,\tval_loss_wass: 0.1617\n",
      "21:\t[0s / 13s],\t\ttrain_loss: 0.2025,\ttrain_loss_classif: 0.2017,\ttrain_loss_wass: 0.0792,\tval_loss: 0.4700,\tval_loss_classif: 0.4684,\tval_loss_wass: 0.1601\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:03:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.775  0.772  0.863  0.866  0.099  0.138  0.107       0.179   \n",
      "lgbm       0.853  0.828  0.857  0.787  0.115  0.194  0.137       0.255   \n",
      "xgb        0.853  0.828  0.842  0.792  0.128  0.185  0.135       0.257   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.192  \n",
      "lgbm            0.250  \n",
      "xgb             0.252  \n",
      "{'wd_param': 0.0, 'wd': 24.866914749145508, 'y_0_perc': 48, 'y_1_perc': 65}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_0.0_wd_24.866914749145508}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.775 &  0.772 &  0.863 &  0.866 & 0.099 &  0.138 &  0.107 &       0.179 &       0.192 \\\\\n",
      " 0.853 &  0.828 &  0.857 &  0.787 & 0.115 &  0.194 &  0.137 &       0.255 &       0.250 \\\\\n",
      " 0.853 &  0.828 &  0.842 &  0.792 & 0.128 &  0.185 &  0.135 &       0.257 &       0.252 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:01<00:00, 40.63s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  76.40813446044922\n",
      "tt = 1 : 51 % \n",
      "Y_0 = 1 : 49 % \n",
      "Y_1 = 1 : 61 % \n",
      "KLD = 0.10091219481865403\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6948,\ttrain_loss_classif: 0.6944,\ttrain_loss_wass: 0.0391,\tval_loss: 0.6947,\tval_loss_classif: 0.6944,\tval_loss_wass: 0.0278\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6908,\ttrain_loss_classif: 0.6904,\ttrain_loss_wass: 0.0357,\tval_loss: 0.6969,\tval_loss_classif: 0.6966,\tval_loss_wass: 0.0223\n",
      "2:\t[0s / 2s],\t\ttrain_loss: 0.6887,\ttrain_loss_classif: 0.6885,\ttrain_loss_wass: 0.0267,\tval_loss: 0.7000,\tval_loss_classif: 0.6998,\tval_loss_wass: 0.0223\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6872,\ttrain_loss_classif: 0.6870,\ttrain_loss_wass: 0.0239,\tval_loss: 0.7051,\tval_loss_classif: 0.7049,\tval_loss_wass: 0.0252\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6852,\ttrain_loss_classif: 0.6848,\ttrain_loss_wass: 0.0390,\tval_loss: 0.7101,\tval_loss_classif: 0.7098,\tval_loss_wass: 0.0268\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 0.6828,\ttrain_loss_classif: 0.6824,\ttrain_loss_wass: 0.0395,\tval_loss: 0.7140,\tval_loss_classif: 0.7137,\tval_loss_wass: 0.0300\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 0.6788,\ttrain_loss_classif: 0.6785,\ttrain_loss_wass: 0.0326,\tval_loss: 0.7133,\tval_loss_classif: 0.7130,\tval_loss_wass: 0.0381\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 0.6724,\ttrain_loss_classif: 0.6720,\ttrain_loss_wass: 0.0407,\tval_loss: 0.7151,\tval_loss_classif: 0.7146,\tval_loss_wass: 0.0520\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:04:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.500  0.619  0.549  0.499  0.157  0.342  0.254       0.362   \n",
      "lgbm       0.794  0.853  0.781  0.780  0.158  0.215  0.181       0.294   \n",
      "xgb        0.794  0.853  0.790  0.798  0.158  0.208  0.176       0.297   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.351  \n",
      "lgbm            0.283  \n",
      "xgb             0.288  \n",
      "{'wd_param': 2.0, 'wd': 76.40813446044922, 'y_0_perc': 49, 'y_1_perc': 61}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_2.0_wd_76.40813446044922}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.500 &  0.619 &  0.549 &  0.499 & 0.157 &  0.342 &  0.254 &       0.362 &       0.351 \\\\\n",
      " 0.794 &  0.853 &  0.781 &  0.780 & 0.158 &  0.215 &  0.181 &       0.294 &       0.283 \\\\\n",
      " 0.794 &  0.853 &  0.790 &  0.798 & 0.158 &  0.208 &  0.176 &       0.297 &       0.288 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:01<02:02, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  75.38691711425781\n",
      "tt = 1 : 51 % \n",
      "Y_0 = 1 : 48 % \n",
      "Y_1 = 1 : 63 % \n",
      "KLD = 0.10295880188182249\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.7000,\ttrain_loss_classif: 0.6992,\ttrain_loss_wass: 0.0724,\tval_loss: 0.6925,\tval_loss_classif: 0.6921,\tval_loss_wass: 0.0411\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6956,\ttrain_loss_classif: 0.6951,\ttrain_loss_wass: 0.0505,\tval_loss: 0.6935,\tval_loss_classif: 0.6931,\tval_loss_wass: 0.0376\n",
      "2:\t[0s / 2s],\t\ttrain_loss: 0.6924,\ttrain_loss_classif: 0.6921,\ttrain_loss_wass: 0.0292,\tval_loss: 0.6953,\tval_loss_classif: 0.6950,\tval_loss_wass: 0.0334\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6894,\ttrain_loss_classif: 0.6891,\ttrain_loss_wass: 0.0363,\tval_loss: 0.6988,\tval_loss_classif: 0.6985,\tval_loss_wass: 0.0336\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6860,\ttrain_loss_classif: 0.6855,\ttrain_loss_wass: 0.0462,\tval_loss: 0.7044,\tval_loss_classif: 0.7041,\tval_loss_wass: 0.0271\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 0.6824,\ttrain_loss_classif: 0.6821,\ttrain_loss_wass: 0.0353,\tval_loss: 0.7109,\tval_loss_classif: 0.7106,\tval_loss_wass: 0.0305\n",
      "6:\t[0s / 5s],\t\ttrain_loss: 0.6804,\ttrain_loss_classif: 0.6799,\ttrain_loss_wass: 0.0570,\tval_loss: 0.7121,\tval_loss_classif: 0.7117,\tval_loss_wass: 0.0346\n",
      "7:\t[1s / 6s],\t\ttrain_loss: 0.6725,\ttrain_loss_classif: 0.6721,\ttrain_loss_wass: 0.0354,\tval_loss: 0.7098,\tval_loss_classif: 0.7093,\tval_loss_wass: 0.0463\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:05:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.519  0.378  0.507  0.500  0.159  0.337  0.246       0.364   \n",
      "lgbm       0.866  0.812  0.836  0.813  0.160  0.193  0.154       0.259   \n",
      "xgb        0.866  0.812  0.826  0.790  0.160  0.209  0.170       0.288   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.345  \n",
      "lgbm            0.249  \n",
      "xgb             0.279  \n",
      "{'wd_param': 2.0, 'wd': 75.38691711425781, 'y_0_perc': 48, 'y_1_perc': 63}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_2.0_wd_75.38691711425781}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.519 &  0.378 &  0.507 &  0.500 & 0.159 &  0.337 &  0.246 &       0.364 &       0.345 \\\\\n",
      " 0.866 &  0.812 &  0.836 &  0.813 & 0.160 &  0.193 &  0.154 &       0.259 &       0.249 \\\\\n",
      " 0.866 &  0.812 &  0.826 &  0.790 & 0.160 &  0.209 &  0.170 &       0.288 &       0.279 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:59<00:59, 59.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  65.29910278320312\n",
      "tt = 1 : 51 % \n",
      "Y_0 = 1 : 51 % \n",
      "Y_1 = 1 : 63 % \n",
      "KLD = 0.09941773649171062\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6947,\ttrain_loss_classif: 0.6941,\ttrain_loss_wass: 0.0553,\tval_loss: 0.6916,\tval_loss_classif: 0.6911,\tval_loss_wass: 0.0529\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6852,\ttrain_loss_classif: 0.6850,\ttrain_loss_wass: 0.0221,\tval_loss: 0.6874,\tval_loss_classif: 0.6869,\tval_loss_wass: 0.0505\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6796,\ttrain_loss_classif: 0.6794,\ttrain_loss_wass: 0.0243,\tval_loss: 0.6855,\tval_loss_classif: 0.6850,\tval_loss_wass: 0.0564\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6745,\ttrain_loss_classif: 0.6742,\ttrain_loss_wass: 0.0352,\tval_loss: 0.6886,\tval_loss_classif: 0.6880,\tval_loss_wass: 0.0612\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 0.6687,\ttrain_loss_classif: 0.6681,\ttrain_loss_wass: 0.0607,\tval_loss: 0.6867,\tval_loss_classif: 0.6861,\tval_loss_wass: 0.0578\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 0.6663,\ttrain_loss_classif: 0.6659,\ttrain_loss_wass: 0.0349,\tval_loss: 0.6816,\tval_loss_classif: 0.6810,\tval_loss_wass: 0.0569\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 0.6633,\ttrain_loss_classif: 0.6631,\ttrain_loss_wass: 0.0250,\tval_loss: 0.6729,\tval_loss_classif: 0.6724,\tval_loss_wass: 0.0533\n",
      "7:\t[0s / 4s],\t\ttrain_loss: 0.6500,\ttrain_loss_classif: 0.6496,\ttrain_loss_wass: 0.0361,\tval_loss: 0.6587,\tval_loss_classif: 0.6582,\tval_loss_wass: 0.0517\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 0.6338,\ttrain_loss_classif: 0.6334,\ttrain_loss_wass: 0.0369,\tval_loss: 0.6328,\tval_loss_classif: 0.6323,\tval_loss_wass: 0.0517\n",
      "9:\t[0s / 5s],\t\ttrain_loss: 0.6018,\ttrain_loss_classif: 0.6014,\ttrain_loss_wass: 0.0420,\tval_loss: 0.5853,\tval_loss_classif: 0.5848,\tval_loss_wass: 0.0498\n",
      "10:\t[0s / 6s],\t\ttrain_loss: 0.5580,\ttrain_loss_classif: 0.5574,\ttrain_loss_wass: 0.0547,\tval_loss: 0.5143,\tval_loss_classif: 0.5137,\tval_loss_wass: 0.0599\n",
      "11:\t[0s / 6s],\t\ttrain_loss: 0.5064,\ttrain_loss_classif: 0.5059,\ttrain_loss_wass: 0.0519,\tval_loss: 0.4593,\tval_loss_classif: 0.4586,\tval_loss_wass: 0.0713\n",
      "12:\t[0s / 7s],\t\ttrain_loss: 0.4668,\ttrain_loss_classif: 0.4661,\ttrain_loss_wass: 0.0701,\tval_loss: 0.3867,\tval_loss_classif: 0.3859,\tval_loss_wass: 0.0807\n",
      "13:\t[0s / 8s],\t\ttrain_loss: 0.4103,\ttrain_loss_classif: 0.4096,\ttrain_loss_wass: 0.0664,\tval_loss: 0.2997,\tval_loss_classif: 0.2989,\tval_loss_wass: 0.0787\n",
      "14:\t[0s / 8s],\t\ttrain_loss: 0.3678,\ttrain_loss_classif: 0.3672,\ttrain_loss_wass: 0.0591,\tval_loss: 0.2443,\tval_loss_classif: 0.2434,\tval_loss_wass: 0.0852\n",
      "15:\t[0s / 9s],\t\ttrain_loss: 0.3438,\ttrain_loss_classif: 0.3432,\ttrain_loss_wass: 0.0639,\tval_loss: 0.2276,\tval_loss_classif: 0.2268,\tval_loss_wass: 0.0840\n",
      "16:\t[0s / 10s],\t\ttrain_loss: 0.3231,\ttrain_loss_classif: 0.3225,\ttrain_loss_wass: 0.0651,\tval_loss: 0.2371,\tval_loss_classif: 0.2362,\tval_loss_wass: 0.0837\n",
      "17:\t[0s / 10s],\t\ttrain_loss: 0.3092,\ttrain_loss_classif: 0.3086,\ttrain_loss_wass: 0.0666,\tval_loss: 0.2324,\tval_loss_classif: 0.2317,\tval_loss_wass: 0.0762\n",
      "18:\t[0s / 11s],\t\ttrain_loss: 0.2952,\ttrain_loss_classif: 0.2945,\ttrain_loss_wass: 0.0696,\tval_loss: 0.2450,\tval_loss_classif: 0.2442,\tval_loss_wass: 0.0762\n",
      "19:\t[0s / 12s],\t\ttrain_loss: 0.2760,\ttrain_loss_classif: 0.2753,\ttrain_loss_wass: 0.0714,\tval_loss: 0.2276,\tval_loss_classif: 0.2269,\tval_loss_wass: 0.0741\n",
      "20:\t[0s / 13s],\t\ttrain_loss: 0.2653,\ttrain_loss_classif: 0.2645,\ttrain_loss_wass: 0.0746,\tval_loss: 0.2269,\tval_loss_classif: 0.2261,\tval_loss_wass: 0.0834\n",
      "21:\t[0s / 13s],\t\ttrain_loss: 0.2618,\ttrain_loss_classif: 0.2611,\ttrain_loss_wass: 0.0649,\tval_loss: 0.2258,\tval_loss_classif: 0.2250,\tval_loss_wass: 0.0830\n",
      "22:\t[0s / 14s],\t\ttrain_loss: 0.2370,\ttrain_loss_classif: 0.2363,\ttrain_loss_wass: 0.0697,\tval_loss: 0.2207,\tval_loss_classif: 0.2199,\tval_loss_wass: 0.0825\n",
      "23:\t[0s / 14s],\t\ttrain_loss: 0.2228,\ttrain_loss_classif: 0.2222,\ttrain_loss_wass: 0.0654,\tval_loss: 0.2329,\tval_loss_classif: 0.2321,\tval_loss_wass: 0.0803\n",
      "24:\t[1s / 15s],\t\ttrain_loss: 0.2057,\ttrain_loss_classif: 0.2050,\ttrain_loss_wass: 0.0715,\tval_loss: 0.2175,\tval_loss_classif: 0.2166,\tval_loss_wass: 0.0822\n",
      "25:\t[0s / 16s],\t\ttrain_loss: 0.1991,\ttrain_loss_classif: 0.1985,\ttrain_loss_wass: 0.0585,\tval_loss: 0.2360,\tval_loss_classif: 0.2352,\tval_loss_wass: 0.0838\n",
      "26:\t[0s / 17s],\t\ttrain_loss: 0.1777,\ttrain_loss_classif: 0.1770,\ttrain_loss_wass: 0.0714,\tval_loss: 0.2266,\tval_loss_classif: 0.2258,\tval_loss_wass: 0.0794\n",
      "27:\t[0s / 18s],\t\ttrain_loss: 0.1640,\ttrain_loss_classif: 0.1634,\ttrain_loss_wass: 0.0606,\tval_loss: 0.2323,\tval_loss_classif: 0.2315,\tval_loss_wass: 0.0798\n",
      "28:\t[0s / 19s],\t\ttrain_loss: 0.1643,\ttrain_loss_classif: 0.1636,\ttrain_loss_wass: 0.0704,\tval_loss: 0.2715,\tval_loss_classif: 0.2707,\tval_loss_wass: 0.0805\n",
      "29:\t[0s / 20s],\t\ttrain_loss: 0.1454,\ttrain_loss_classif: 0.1448,\ttrain_loss_wass: 0.0643,\tval_loss: 0.2476,\tval_loss_classif: 0.2467,\tval_loss_wass: 0.0880\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:06:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.781  0.834  0.866  0.923  0.138  0.124  0.123       0.202   \n",
      "lgbm       0.831  0.856  0.828  0.840  0.158  0.179  0.128       0.237   \n",
      "xgb        0.831  0.856  0.815  0.845  0.158  0.177  0.145       0.252   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.197  \n",
      "lgbm            0.224  \n",
      "xgb             0.245  \n",
      "{'wd_param': 2.0, 'wd': 65.29910278320312, 'y_0_perc': 51, 'y_1_perc': 63}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_2.0_wd_65.29910278320312}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.781 &  0.834 &  0.866 &  0.923 & 0.138 &  0.124 &  0.123 &       0.202 &       0.197 \\\\\n",
      " 0.831 &  0.856 &  0.828 &  0.840 & 0.158 &  0.179 &  0.128 &       0.237 &       0.224 \\\\\n",
      " 0.831 &  0.856 &  0.815 &  0.845 & 0.158 &  0.177 &  0.145 &       0.252 &       0.245 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:10<00:00, 63.48s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  126.32049560546875\n",
      "tt = 1 : 50 % \n",
      "Y_0 = 1 : 48 % \n",
      "Y_1 = 1 : 60 % \n",
      "KLD = 0.10087270260162602\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6990,\ttrain_loss_classif: 0.6983,\ttrain_loss_wass: 0.0654,\tval_loss: 0.6989,\tval_loss_classif: 0.6985,\tval_loss_wass: 0.0358\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 0.6943,\ttrain_loss_classif: 0.6940,\ttrain_loss_wass: 0.0349,\tval_loss: 0.6945,\tval_loss_classif: 0.6941,\tval_loss_wass: 0.0362\n",
      "2:\t[1s / 3s],\t\ttrain_loss: 0.6910,\ttrain_loss_classif: 0.6905,\ttrain_loss_wass: 0.0426,\tval_loss: 0.6917,\tval_loss_classif: 0.6914,\tval_loss_wass: 0.0329\n",
      "3:\t[1s / 4s],\t\ttrain_loss: 0.6863,\ttrain_loss_classif: 0.6859,\ttrain_loss_wass: 0.0381,\tval_loss: 0.6862,\tval_loss_classif: 0.6860,\tval_loss_wass: 0.0233\n",
      "4:\t[1s / 5s],\t\ttrain_loss: 0.6820,\ttrain_loss_classif: 0.6813,\ttrain_loss_wass: 0.0687,\tval_loss: 0.6799,\tval_loss_classif: 0.6793,\tval_loss_wass: 0.0534\n",
      "5:\t[1s / 7s],\t\ttrain_loss: 0.6736,\ttrain_loss_classif: 0.6722,\ttrain_loss_wass: 0.1354,\tval_loss: 0.6740,\tval_loss_classif: 0.6734,\tval_loss_wass: 0.0626\n",
      "6:\t[2s / 9s],\t\ttrain_loss: 0.6617,\ttrain_loss_classif: 0.6606,\ttrain_loss_wass: 0.1146,\tval_loss: 0.6657,\tval_loss_classif: 0.6653,\tval_loss_wass: 0.0466\n",
      "7:\t[1s / 11s],\t\ttrain_loss: 0.6455,\ttrain_loss_classif: 0.6448,\ttrain_loss_wass: 0.0656,\tval_loss: 0.6491,\tval_loss_classif: 0.6487,\tval_loss_wass: 0.0410\n",
      "8:\t[0s / 11s],\t\ttrain_loss: 0.6244,\ttrain_loss_classif: 0.6239,\ttrain_loss_wass: 0.0481,\tval_loss: 0.6194,\tval_loss_classif: 0.6189,\tval_loss_wass: 0.0482\n",
      "9:\t[0s / 12s],\t\ttrain_loss: 0.5871,\ttrain_loss_classif: 0.5865,\ttrain_loss_wass: 0.0638,\tval_loss: 0.5682,\tval_loss_classif: 0.5676,\tval_loss_wass: 0.0643\n",
      "10:\t[1s / 13s],\t\ttrain_loss: 0.5339,\ttrain_loss_classif: 0.5331,\ttrain_loss_wass: 0.0793,\tval_loss: 0.5671,\tval_loss_classif: 0.5662,\tval_loss_wass: 0.0950\n",
      "11:\t[0s / 14s],\t\ttrain_loss: 0.4954,\ttrain_loss_classif: 0.4946,\ttrain_loss_wass: 0.0822,\tval_loss: 0.5021,\tval_loss_classif: 0.5011,\tval_loss_wass: 0.0952\n",
      "12:\t[1s / 15s],\t\ttrain_loss: 0.4431,\ttrain_loss_classif: 0.4421,\ttrain_loss_wass: 0.0994,\tval_loss: 0.5771,\tval_loss_classif: 0.5761,\tval_loss_wass: 0.0995\n",
      "13:\t[0s / 16s],\t\ttrain_loss: 0.4237,\ttrain_loss_classif: 0.4228,\ttrain_loss_wass: 0.0874,\tval_loss: 0.4882,\tval_loss_classif: 0.4872,\tval_loss_wass: 0.0994\n",
      "14:\t[0s / 17s],\t\ttrain_loss: 0.3924,\ttrain_loss_classif: 0.3914,\ttrain_loss_wass: 0.0980,\tval_loss: 0.5191,\tval_loss_classif: 0.5182,\tval_loss_wass: 0.0863\n",
      "15:\t[0s / 18s],\t\ttrain_loss: 0.3971,\ttrain_loss_classif: 0.3962,\ttrain_loss_wass: 0.0889,\tval_loss: 0.4733,\tval_loss_classif: 0.4726,\tval_loss_wass: 0.0721\n",
      "16:\t[0s / 19s],\t\ttrain_loss: 0.3815,\ttrain_loss_classif: 0.3808,\ttrain_loss_wass: 0.0761,\tval_loss: 0.4561,\tval_loss_classif: 0.4553,\tval_loss_wass: 0.0795\n",
      "17:\t[0s / 19s],\t\ttrain_loss: 0.3739,\ttrain_loss_classif: 0.3732,\ttrain_loss_wass: 0.0713,\tval_loss: 0.4737,\tval_loss_classif: 0.4729,\tval_loss_wass: 0.0775\n",
      "18:\t[0s / 20s],\t\ttrain_loss: 0.3476,\ttrain_loss_classif: 0.3469,\ttrain_loss_wass: 0.0658,\tval_loss: 0.4547,\tval_loss_classif: 0.4539,\tval_loss_wass: 0.0781\n",
      "19:\t[0s / 20s],\t\ttrain_loss: 0.3286,\ttrain_loss_classif: 0.3279,\ttrain_loss_wass: 0.0707,\tval_loss: 0.4600,\tval_loss_classif: 0.4592,\tval_loss_wass: 0.0813\n",
      "20:\t[0s / 21s],\t\ttrain_loss: 0.3116,\ttrain_loss_classif: 0.3109,\ttrain_loss_wass: 0.0767,\tval_loss: 0.4640,\tval_loss_classif: 0.4631,\tval_loss_wass: 0.0835\n",
      "21:\t[0s / 22s],\t\ttrain_loss: 0.2984,\ttrain_loss_classif: 0.2977,\ttrain_loss_wass: 0.0748,\tval_loss: 0.4735,\tval_loss_classif: 0.4728,\tval_loss_wass: 0.0777\n",
      "22:\t[0s / 23s],\t\ttrain_loss: 0.2889,\ttrain_loss_classif: 0.2882,\ttrain_loss_wass: 0.0664,\tval_loss: 0.4856,\tval_loss_classif: 0.4848,\tval_loss_wass: 0.0817\n",
      "23:\t[0s / 23s],\t\ttrain_loss: 0.2919,\ttrain_loss_classif: 0.2913,\ttrain_loss_wass: 0.0660,\tval_loss: 0.4785,\tval_loss_classif: 0.4776,\tval_loss_wass: 0.0877\n",
      "24:\t[0s / 24s],\t\ttrain_loss: 0.2710,\ttrain_loss_classif: 0.2703,\ttrain_loss_wass: 0.0686,\tval_loss: 0.4844,\tval_loss_classif: 0.4837,\tval_loss_wass: 0.0725\n",
      "25:\t[0s / 25s],\t\ttrain_loss: 0.2795,\ttrain_loss_classif: 0.2786,\ttrain_loss_wass: 0.0870,\tval_loss: 0.4622,\tval_loss_classif: 0.4615,\tval_loss_wass: 0.0745\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:07:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.612  0.778  0.856  0.855  0.429  0.255  0.173       0.299   \n",
      "lgbm       0.844  0.878  0.850  0.867  0.148  0.195  0.173       0.273   \n",
      "xgb        0.844  0.878  0.843  0.869  0.148  0.193  0.160       0.284   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.247  \n",
      "lgbm            0.269  \n",
      "xgb             0.277  \n",
      "{'wd_param': 4.0, 'wd': 126.32049560546875, 'y_0_perc': 48, 'y_1_perc': 60}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_4.0_wd_126.32049560546875}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.612 &  0.778 &  0.856 &  0.855 & 0.429 &  0.255 &  0.173 &       0.299 &       0.247 \\\\\n",
      " 0.844 &  0.878 &  0.850 &  0.867 & 0.148 &  0.195 &  0.173 &       0.273 &       0.269 \\\\\n",
      " 0.844 &  0.878 &  0.843 &  0.869 & 0.148 &  0.193 &  0.160 &       0.284 &       0.277 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:20<02:41, 80.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  278.48052978515625\n",
      "tt = 1 : 49 % \n",
      "Y_0 = 1 : 49 % \n",
      "Y_1 = 1 : 62 % \n",
      "KLD = 0.10123175786251101\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6888,\ttrain_loss_classif: 0.6881,\ttrain_loss_wass: 0.0721,\tval_loss: 0.6872,\tval_loss_classif: 0.6867,\tval_loss_wass: 0.0442\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6862,\ttrain_loss_classif: 0.6858,\ttrain_loss_wass: 0.0357,\tval_loss: 0.6838,\tval_loss_classif: 0.6833,\tval_loss_wass: 0.0484\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6834,\ttrain_loss_classif: 0.6827,\ttrain_loss_wass: 0.0656,\tval_loss: 0.6790,\tval_loss_classif: 0.6784,\tval_loss_wass: 0.0567\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6813,\ttrain_loss_classif: 0.6807,\ttrain_loss_wass: 0.0551,\tval_loss: 0.6729,\tval_loss_classif: 0.6722,\tval_loss_wass: 0.0681\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6763,\ttrain_loss_classif: 0.6758,\ttrain_loss_wass: 0.0486,\tval_loss: 0.6651,\tval_loss_classif: 0.6645,\tval_loss_wass: 0.0590\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 0.6662,\ttrain_loss_classif: 0.6656,\ttrain_loss_wass: 0.0638,\tval_loss: 0.6522,\tval_loss_classif: 0.6516,\tval_loss_wass: 0.0613\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 0.6505,\ttrain_loss_classif: 0.6499,\ttrain_loss_wass: 0.0630,\tval_loss: 0.6301,\tval_loss_classif: 0.6294,\tval_loss_wass: 0.0625\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 0.6189,\ttrain_loss_classif: 0.6183,\ttrain_loss_wass: 0.0647,\tval_loss: 0.5860,\tval_loss_classif: 0.5852,\tval_loss_wass: 0.0798\n",
      "8:\t[0s / 6s],\t\ttrain_loss: 0.5778,\ttrain_loss_classif: 0.5769,\ttrain_loss_wass: 0.0922,\tval_loss: 0.5268,\tval_loss_classif: 0.5258,\tval_loss_wass: 0.0926\n",
      "9:\t[1s / 7s],\t\ttrain_loss: 0.5260,\ttrain_loss_classif: 0.5250,\ttrain_loss_wass: 0.1070,\tval_loss: 0.4686,\tval_loss_classif: 0.4676,\tval_loss_wass: 0.0987\n",
      "10:\t[0s / 7s],\t\ttrain_loss: 0.4699,\ttrain_loss_classif: 0.4689,\ttrain_loss_wass: 0.0976,\tval_loss: 0.4323,\tval_loss_classif: 0.4313,\tval_loss_wass: 0.1031\n",
      "11:\t[0s / 8s],\t\ttrain_loss: 0.4454,\ttrain_loss_classif: 0.4442,\ttrain_loss_wass: 0.1233,\tval_loss: 0.4106,\tval_loss_classif: 0.4095,\tval_loss_wass: 0.1140\n",
      "12:\t[0s / 9s],\t\ttrain_loss: 0.4248,\ttrain_loss_classif: 0.4236,\ttrain_loss_wass: 0.1166,\tval_loss: 0.4097,\tval_loss_classif: 0.4087,\tval_loss_wass: 0.1068\n",
      "13:\t[0s / 9s],\t\ttrain_loss: 0.4128,\ttrain_loss_classif: 0.4117,\ttrain_loss_wass: 0.1124,\tval_loss: 0.3732,\tval_loss_classif: 0.3721,\tval_loss_wass: 0.1007\n",
      "14:\t[0s / 10s],\t\ttrain_loss: 0.3883,\ttrain_loss_classif: 0.3872,\ttrain_loss_wass: 0.1072,\tval_loss: 0.3665,\tval_loss_classif: 0.3656,\tval_loss_wass: 0.0976\n",
      "15:\t[0s / 10s],\t\ttrain_loss: 0.3499,\ttrain_loss_classif: 0.3487,\ttrain_loss_wass: 0.1197,\tval_loss: 0.3612,\tval_loss_classif: 0.3602,\tval_loss_wass: 0.1012\n",
      "16:\t[0s / 11s],\t\ttrain_loss: 0.3387,\ttrain_loss_classif: 0.3376,\ttrain_loss_wass: 0.1046,\tval_loss: 0.3631,\tval_loss_classif: 0.3621,\tval_loss_wass: 0.1005\n",
      "17:\t[0s / 12s],\t\ttrain_loss: 0.3217,\ttrain_loss_classif: 0.3206,\ttrain_loss_wass: 0.1036,\tval_loss: 0.3611,\tval_loss_classif: 0.3601,\tval_loss_wass: 0.1023\n",
      "18:\t[0s / 13s],\t\ttrain_loss: 0.3129,\ttrain_loss_classif: 0.3119,\ttrain_loss_wass: 0.0934,\tval_loss: 0.3662,\tval_loss_classif: 0.3652,\tval_loss_wass: 0.0990\n",
      "19:\t[0s / 13s],\t\ttrain_loss: 0.2948,\ttrain_loss_classif: 0.2937,\ttrain_loss_wass: 0.1048,\tval_loss: 0.3570,\tval_loss_classif: 0.3560,\tval_loss_wass: 0.0999\n",
      "20:\t[0s / 14s],\t\ttrain_loss: 0.2831,\ttrain_loss_classif: 0.2819,\ttrain_loss_wass: 0.1159,\tval_loss: 0.3625,\tval_loss_classif: 0.3615,\tval_loss_wass: 0.1024\n",
      "21:\t[0s / 15s],\t\ttrain_loss: 0.2765,\ttrain_loss_classif: 0.2753,\ttrain_loss_wass: 0.1240,\tval_loss: 0.3781,\tval_loss_classif: 0.3771,\tval_loss_wass: 0.1058\n",
      "22:\t[0s / 16s],\t\ttrain_loss: 0.2608,\ttrain_loss_classif: 0.2598,\ttrain_loss_wass: 0.0996,\tval_loss: 0.3855,\tval_loss_classif: 0.3845,\tval_loss_wass: 0.1064\n",
      "23:\t[0s / 16s],\t\ttrain_loss: 0.2753,\ttrain_loss_classif: 0.2742,\ttrain_loss_wass: 0.1072,\tval_loss: 0.4835,\tval_loss_classif: 0.4825,\tval_loss_wass: 0.1032\n",
      "24:\t[0s / 17s],\t\ttrain_loss: 0.2894,\ttrain_loss_classif: 0.2881,\ttrain_loss_wass: 0.1292,\tval_loss: 0.4556,\tval_loss_classif: 0.4546,\tval_loss_wass: 0.1015\n",
      "25:\t[0s / 17s],\t\ttrain_loss: 0.3134,\ttrain_loss_classif: 0.3125,\ttrain_loss_wass: 0.0942,\tval_loss: 0.4014,\tval_loss_classif: 0.4004,\tval_loss_wass: 0.1009\n",
      "26:\t[0s / 18s],\t\ttrain_loss: 0.2600,\ttrain_loss_classif: 0.2589,\ttrain_loss_wass: 0.1015,\tval_loss: 0.3884,\tval_loss_classif: 0.3874,\tval_loss_wass: 0.1014\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:09:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.738  0.666  0.817  0.792  0.381  0.187  0.281       0.256   \n",
      "lgbm       0.828  0.822  0.799  0.833  0.156  0.186  0.182       0.261   \n",
      "xgb        0.828  0.822  0.808  0.835  0.156  0.182  0.194       0.265   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.287  \n",
      "lgbm            0.259  \n",
      "xgb             0.265  \n",
      "{'wd_param': 4.0, 'wd': 278.48052978515625, 'y_0_perc': 49, 'y_1_perc': 62}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_4.0_wd_278.48052978515625}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.738 &  0.666 &  0.817 &  0.792 & 0.381 &  0.187 &  0.281 &       0.256 &       0.287 \\\\\n",
      " 0.828 &  0.822 &  0.799 &  0.833 & 0.156 &  0.186 &  0.182 &       0.261 &       0.259 \\\\\n",
      " 0.828 &  0.822 &  0.808 &  0.835 & 0.156 &  0.182 &  0.194 &       0.265 &       0.265 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [02:24<01:10, 70.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  159.47384643554688\n",
      "tt = 1 : 51 % \n",
      "Y_0 = 1 : 47 % \n",
      "Y_1 = 1 : 60 % \n",
      "KLD = 0.10508164137758055\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6986,\ttrain_loss_classif: 0.6976,\ttrain_loss_wass: 0.1059,\tval_loss: 0.6933,\tval_loss_classif: 0.6928,\tval_loss_wass: 0.0552\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6912,\ttrain_loss_classif: 0.6908,\ttrain_loss_wass: 0.0433,\tval_loss: 0.6887,\tval_loss_classif: 0.6884,\tval_loss_wass: 0.0297\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6864,\ttrain_loss_classif: 0.6861,\ttrain_loss_wass: 0.0279,\tval_loss: 0.6859,\tval_loss_classif: 0.6856,\tval_loss_wass: 0.0309\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6841,\ttrain_loss_classif: 0.6836,\ttrain_loss_wass: 0.0449,\tval_loss: 0.6876,\tval_loss_classif: 0.6870,\tval_loss_wass: 0.0581\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6863,\ttrain_loss_classif: 0.6857,\ttrain_loss_wass: 0.0552,\tval_loss: 0.6900,\tval_loss_classif: 0.6894,\tval_loss_wass: 0.0607\n",
      "5:\t[0s / 3s],\t\ttrain_loss: 0.6835,\ttrain_loss_classif: 0.6832,\ttrain_loss_wass: 0.0251,\tval_loss: 0.6900,\tval_loss_classif: 0.6894,\tval_loss_wass: 0.0618\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 0.6804,\ttrain_loss_classif: 0.6798,\ttrain_loss_wass: 0.0550,\tval_loss: 0.6908,\tval_loss_classif: 0.6904,\tval_loss_wass: 0.0424\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 0.6759,\ttrain_loss_classif: 0.6754,\ttrain_loss_wass: 0.0546,\tval_loss: 0.6915,\tval_loss_classif: 0.6910,\tval_loss_wass: 0.0436\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 0.6690,\ttrain_loss_classif: 0.6683,\ttrain_loss_wass: 0.0641,\tval_loss: 0.6907,\tval_loss_classif: 0.6903,\tval_loss_wass: 0.0402\n",
      "9:\t[0s / 6s],\t\ttrain_loss: 0.6600,\ttrain_loss_classif: 0.6595,\ttrain_loss_wass: 0.0499,\tval_loss: 0.6792,\tval_loss_classif: 0.6789,\tval_loss_wass: 0.0348\n",
      "10:\t[0s / 7s],\t\ttrain_loss: 0.6408,\ttrain_loss_classif: 0.6404,\ttrain_loss_wass: 0.0456,\tval_loss: 0.6581,\tval_loss_classif: 0.6577,\tval_loss_wass: 0.0356\n",
      "11:\t[0s / 7s],\t\ttrain_loss: 0.6082,\ttrain_loss_classif: 0.6078,\ttrain_loss_wass: 0.0401,\tval_loss: 0.6268,\tval_loss_classif: 0.6264,\tval_loss_wass: 0.0348\n",
      "12:\t[0s / 8s],\t\ttrain_loss: 0.5669,\ttrain_loss_classif: 0.5665,\ttrain_loss_wass: 0.0432,\tval_loss: 0.6258,\tval_loss_classif: 0.6254,\tval_loss_wass: 0.0419\n",
      "13:\t[0s / 9s],\t\ttrain_loss: 0.5434,\ttrain_loss_classif: 0.5428,\ttrain_loss_wass: 0.0604,\tval_loss: 0.5942,\tval_loss_classif: 0.5938,\tval_loss_wass: 0.0470\n",
      "14:\t[0s / 9s],\t\ttrain_loss: 0.5212,\ttrain_loss_classif: 0.5207,\ttrain_loss_wass: 0.0505,\tval_loss: 0.6140,\tval_loss_classif: 0.6136,\tval_loss_wass: 0.0442\n",
      "15:\t[0s / 10s],\t\ttrain_loss: 0.4853,\ttrain_loss_classif: 0.4849,\ttrain_loss_wass: 0.0465,\tval_loss: 0.5995,\tval_loss_classif: 0.5990,\tval_loss_wass: 0.0417\n",
      "16:\t[0s / 10s],\t\ttrain_loss: 0.4548,\ttrain_loss_classif: 0.4543,\ttrain_loss_wass: 0.0473,\tval_loss: 0.6222,\tval_loss_classif: 0.6217,\tval_loss_wass: 0.0470\n",
      "17:\t[0s / 11s],\t\ttrain_loss: 0.4398,\ttrain_loss_classif: 0.4393,\ttrain_loss_wass: 0.0558,\tval_loss: 0.6214,\tval_loss_classif: 0.6209,\tval_loss_wass: 0.0505\n",
      "18:\t[0s / 12s],\t\ttrain_loss: 0.4308,\ttrain_loss_classif: 0.4301,\ttrain_loss_wass: 0.0689,\tval_loss: 0.5740,\tval_loss_classif: 0.5735,\tval_loss_wass: 0.0523\n",
      "19:\t[0s / 12s],\t\ttrain_loss: 0.4286,\ttrain_loss_classif: 0.4279,\ttrain_loss_wass: 0.0714,\tval_loss: 0.5661,\tval_loss_classif: 0.5656,\tval_loss_wass: 0.0513\n",
      "20:\t[0s / 13s],\t\ttrain_loss: 0.3993,\ttrain_loss_classif: 0.3987,\ttrain_loss_wass: 0.0685,\tval_loss: 0.5349,\tval_loss_classif: 0.5344,\tval_loss_wass: 0.0516\n",
      "21:\t[0s / 14s],\t\ttrain_loss: 0.3839,\ttrain_loss_classif: 0.3832,\ttrain_loss_wass: 0.0642,\tval_loss: 0.5498,\tval_loss_classif: 0.5493,\tval_loss_wass: 0.0513\n",
      "22:\t[0s / 14s],\t\ttrain_loss: 0.3650,\ttrain_loss_classif: 0.3643,\ttrain_loss_wass: 0.0647,\tval_loss: 0.5242,\tval_loss_classif: 0.5236,\tval_loss_wass: 0.0576\n",
      "23:\t[0s / 15s],\t\ttrain_loss: 0.3607,\ttrain_loss_classif: 0.3600,\ttrain_loss_wass: 0.0696,\tval_loss: 0.5680,\tval_loss_classif: 0.5674,\tval_loss_wass: 0.0596\n",
      "24:\t[0s / 15s],\t\ttrain_loss: 0.3468,\ttrain_loss_classif: 0.3461,\ttrain_loss_wass: 0.0679,\tval_loss: 0.5079,\tval_loss_classif: 0.5073,\tval_loss_wass: 0.0636\n",
      "25:\t[0s / 16s],\t\ttrain_loss: 0.3535,\ttrain_loss_classif: 0.3527,\ttrain_loss_wass: 0.0787,\tval_loss: 0.5433,\tval_loss_classif: 0.5427,\tval_loss_wass: 0.0585\n",
      "26:\t[0s / 16s],\t\ttrain_loss: 0.3384,\ttrain_loss_classif: 0.3376,\ttrain_loss_wass: 0.0797,\tval_loss: 0.5352,\tval_loss_classif: 0.5346,\tval_loss_wass: 0.0666\n",
      "27:\t[0s / 17s],\t\ttrain_loss: 0.3192,\ttrain_loss_classif: 0.3185,\ttrain_loss_wass: 0.0675,\tval_loss: 0.5233,\tval_loss_classif: 0.5227,\tval_loss_wass: 0.0570\n",
      "28:\t[0s / 17s],\t\ttrain_loss: 0.3096,\ttrain_loss_classif: 0.3090,\ttrain_loss_wass: 0.0575,\tval_loss: 0.5644,\tval_loss_classif: 0.5638,\tval_loss_wass: 0.0611\n",
      "29:\t[0s / 18s],\t\ttrain_loss: 0.2952,\ttrain_loss_classif: 0.2944,\ttrain_loss_wass: 0.0802,\tval_loss: 0.5277,\tval_loss_classif: 0.5271,\tval_loss_wass: 0.0639\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:10:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.666  0.719  0.828  0.784  0.394  0.241  0.279       0.290   \n",
      "lgbm       0.853  0.812  0.761  0.767  0.163  0.224  0.187       0.294   \n",
      "xgb        0.853  0.812  0.764  0.771  0.163  0.221  0.202       0.295   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.305  \n",
      "lgbm            0.294  \n",
      "xgb             0.298  \n",
      "{'wd_param': 4.0, 'wd': 159.47384643554688, 'y_0_perc': 47, 'y_1_perc': 60}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_4.0_wd_159.47384643554688}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.666 &  0.719 &  0.828 &  0.784 & 0.394 &  0.241 &  0.279 &       0.290 &       0.305 \\\\\n",
      " 0.853 &  0.812 &  0.761 &  0.767 & 0.163 &  0.224 &  0.187 &       0.294 &       0.294 \\\\\n",
      " 0.853 &  0.812 &  0.764 &  0.771 & 0.163 &  0.221 &  0.202 &       0.295 &       0.298 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:23<00:00, 67.72s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  708.8859252929688\n",
      "tt = 1 : 48 % \n",
      "Y_0 = 1 : 48 % \n",
      "Y_1 = 1 : 61 % \n",
      "KLD = 0.09623471717646459\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6892,\ttrain_loss_classif: 0.6871,\ttrain_loss_wass: 0.2058,\tval_loss: 0.6797,\tval_loss_classif: 0.6786,\tval_loss_wass: 0.1057\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6830,\ttrain_loss_classif: 0.6819,\ttrain_loss_wass: 0.1078,\tval_loss: 0.6723,\tval_loss_classif: 0.6712,\tval_loss_wass: 0.1149\n",
      "2:\t[0s / 2s],\t\ttrain_loss: 0.6765,\ttrain_loss_classif: 0.6753,\ttrain_loss_wass: 0.1180,\tval_loss: 0.6604,\tval_loss_classif: 0.6591,\tval_loss_wass: 0.1225\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6676,\ttrain_loss_classif: 0.6661,\ttrain_loss_wass: 0.1555,\tval_loss: 0.6457,\tval_loss_classif: 0.6440,\tval_loss_wass: 0.1678\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6552,\ttrain_loss_classif: 0.6532,\ttrain_loss_wass: 0.1935,\tval_loss: 0.6298,\tval_loss_classif: 0.6281,\tval_loss_wass: 0.1630\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 0.6445,\ttrain_loss_classif: 0.6426,\ttrain_loss_wass: 0.1947,\tval_loss: 0.6214,\tval_loss_classif: 0.6205,\tval_loss_wass: 0.0960\n",
      "6:\t[0s / 5s],\t\ttrain_loss: 0.6387,\ttrain_loss_classif: 0.6375,\ttrain_loss_wass: 0.1200,\tval_loss: 0.6094,\tval_loss_classif: 0.6086,\tval_loss_wass: 0.0793\n",
      "7:\t[0s / 6s],\t\ttrain_loss: 0.6223,\ttrain_loss_classif: 0.6210,\ttrain_loss_wass: 0.1268,\tval_loss: 0.6064,\tval_loss_classif: 0.6053,\tval_loss_wass: 0.1160\n",
      "8:\t[0s / 6s],\t\ttrain_loss: 0.6002,\ttrain_loss_classif: 0.5987,\ttrain_loss_wass: 0.1508,\tval_loss: 0.5613,\tval_loss_classif: 0.5599,\tval_loss_wass: 0.1374\n",
      "9:\t[0s / 7s],\t\ttrain_loss: 0.5672,\ttrain_loss_classif: 0.5653,\ttrain_loss_wass: 0.1928,\tval_loss: 0.5752,\tval_loss_classif: 0.5733,\tval_loss_wass: 0.1897\n",
      "10:\t[1s / 8s],\t\ttrain_loss: 0.5181,\ttrain_loss_classif: 0.5160,\ttrain_loss_wass: 0.2161,\tval_loss: 0.5047,\tval_loss_classif: 0.5030,\tval_loss_wass: 0.1657\n",
      "11:\t[0s / 9s],\t\ttrain_loss: 0.4751,\ttrain_loss_classif: 0.4732,\ttrain_loss_wass: 0.1912,\tval_loss: 0.4649,\tval_loss_classif: 0.4634,\tval_loss_wass: 0.1483\n",
      "12:\t[0s / 10s],\t\ttrain_loss: 0.4666,\ttrain_loss_classif: 0.4652,\ttrain_loss_wass: 0.1481,\tval_loss: 0.5639,\tval_loss_classif: 0.5623,\tval_loss_wass: 0.1554\n",
      "13:\t[0s / 11s],\t\ttrain_loss: 0.4514,\ttrain_loss_classif: 0.4499,\ttrain_loss_wass: 0.1508,\tval_loss: 0.4318,\tval_loss_classif: 0.4306,\tval_loss_wass: 0.1252\n",
      "14:\t[0s / 11s],\t\ttrain_loss: 0.4101,\ttrain_loss_classif: 0.4085,\ttrain_loss_wass: 0.1572,\tval_loss: 0.4721,\tval_loss_classif: 0.4709,\tval_loss_wass: 0.1163\n",
      "15:\t[0s / 12s],\t\ttrain_loss: 0.3946,\ttrain_loss_classif: 0.3932,\ttrain_loss_wass: 0.1447,\tval_loss: 0.4730,\tval_loss_classif: 0.4718,\tval_loss_wass: 0.1215\n",
      "16:\t[0s / 13s],\t\ttrain_loss: 0.3810,\ttrain_loss_classif: 0.3797,\ttrain_loss_wass: 0.1309,\tval_loss: 0.4129,\tval_loss_classif: 0.4118,\tval_loss_wass: 0.1101\n",
      "17:\t[0s / 13s],\t\ttrain_loss: 0.3579,\ttrain_loss_classif: 0.3563,\ttrain_loss_wass: 0.1598,\tval_loss: 0.4168,\tval_loss_classif: 0.4157,\tval_loss_wass: 0.1137\n",
      "18:\t[0s / 14s],\t\ttrain_loss: 0.3647,\ttrain_loss_classif: 0.3634,\ttrain_loss_wass: 0.1293,\tval_loss: 0.4707,\tval_loss_classif: 0.4695,\tval_loss_wass: 0.1206\n",
      "19:\t[0s / 15s],\t\ttrain_loss: 0.3562,\ttrain_loss_classif: 0.3547,\ttrain_loss_wass: 0.1415,\tval_loss: 0.4358,\tval_loss_classif: 0.4348,\tval_loss_wass: 0.1046\n",
      "20:\t[0s / 15s],\t\ttrain_loss: 0.3218,\ttrain_loss_classif: 0.3206,\ttrain_loss_wass: 0.1251,\tval_loss: 0.3970,\tval_loss_classif: 0.3959,\tval_loss_wass: 0.1049\n",
      "21:\t[0s / 16s],\t\ttrain_loss: 0.3032,\ttrain_loss_classif: 0.3021,\ttrain_loss_wass: 0.1185,\tval_loss: 0.3924,\tval_loss_classif: 0.3914,\tval_loss_wass: 0.1022\n",
      "22:\t[0s / 16s],\t\ttrain_loss: 0.2931,\ttrain_loss_classif: 0.2918,\ttrain_loss_wass: 0.1268,\tval_loss: 0.4066,\tval_loss_classif: 0.4056,\tval_loss_wass: 0.1059\n",
      "23:\t[0s / 17s],\t\ttrain_loss: 0.2959,\ttrain_loss_classif: 0.2944,\ttrain_loss_wass: 0.1478,\tval_loss: 0.4507,\tval_loss_classif: 0.4495,\tval_loss_wass: 0.1171\n",
      "24:\t[0s / 18s],\t\ttrain_loss: 0.2817,\ttrain_loss_classif: 0.2803,\ttrain_loss_wass: 0.1379,\tval_loss: 0.4233,\tval_loss_classif: 0.4222,\tval_loss_wass: 0.1152\n",
      "25:\t[0s / 18s],\t\ttrain_loss: 0.2692,\ttrain_loss_classif: 0.2676,\ttrain_loss_wass: 0.1510,\tval_loss: 0.4560,\tval_loss_classif: 0.4550,\tval_loss_wass: 0.0972\n",
      "26:\t[0s / 19s],\t\ttrain_loss: 0.2694,\ttrain_loss_classif: 0.2681,\ttrain_loss_wass: 0.1300,\tval_loss: 0.5018,\tval_loss_classif: 0.5006,\tval_loss_wass: 0.1155\n",
      "27:\t[0s / 19s],\t\ttrain_loss: 0.2858,\ttrain_loss_classif: 0.2845,\ttrain_loss_wass: 0.1266,\tval_loss: 0.4371,\tval_loss_classif: 0.4361,\tval_loss_wass: 0.1015\n",
      "28:\t[0s / 20s],\t\ttrain_loss: 0.2597,\ttrain_loss_classif: 0.2584,\ttrain_loss_wass: 0.1307,\tval_loss: 0.4992,\tval_loss_classif: 0.4981,\tval_loss_wass: 0.1102\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:11:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.638  0.656  0.784  0.783  0.553  0.270  0.281       0.343   \n",
      "lgbm       0.838  0.847  0.862  0.865  0.140  0.163  0.157       0.255   \n",
      "xgb        0.838  0.847  0.862  0.868  0.140  0.156  0.167       0.250   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.322  \n",
      "lgbm            0.254  \n",
      "xgb             0.252  \n",
      "{'wd_param': 6.0, 'wd': 708.8859252929688, 'y_0_perc': 48, 'y_1_perc': 61}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_6.0_wd_708.8859252929688}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.638 &  0.656 &  0.784 &  0.783 & 0.553 &  0.270 &  0.281 &       0.343 &       0.322 \\\\\n",
      " 0.838 &  0.847 &  0.862 &  0.865 & 0.140 &  0.163 &  0.157 &       0.255 &       0.254 \\\\\n",
      " 0.838 &  0.847 &  0.862 &  0.868 & 0.140 &  0.156 &  0.167 &       0.250 &       0.252 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:00<02:00, 60.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  273.0181579589844\n",
      "tt = 1 : 51 % \n",
      "Y_0 = 1 : 46 % \n",
      "Y_1 = 1 : 61 % \n",
      "KLD = 0.10213685261278688\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6976,\ttrain_loss_classif: 0.6955,\ttrain_loss_wass: 0.2101,\tval_loss: 0.6905,\tval_loss_classif: 0.6895,\tval_loss_wass: 0.1022\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6865,\ttrain_loss_classif: 0.6853,\ttrain_loss_wass: 0.1232,\tval_loss: 0.6800,\tval_loss_classif: 0.6781,\tval_loss_wass: 0.1894\n",
      "2:\t[0s / 2s],\t\ttrain_loss: 0.6713,\ttrain_loss_classif: 0.6686,\ttrain_loss_wass: 0.2686,\tval_loss: 0.6617,\tval_loss_classif: 0.6576,\tval_loss_wass: 0.4122\n",
      "3:\t[1s / 3s],\t\ttrain_loss: 0.6500,\ttrain_loss_classif: 0.6436,\ttrain_loss_wass: 0.6409,\tval_loss: 0.6342,\tval_loss_classif: 0.6296,\tval_loss_wass: 0.4536\n",
      "4:\t[1s / 4s],\t\ttrain_loss: 0.6343,\ttrain_loss_classif: 0.6244,\ttrain_loss_wass: 0.9899,\tval_loss: 0.6248,\tval_loss_classif: 0.6208,\tval_loss_wass: 0.4064\n",
      "5:\t[1s / 6s],\t\ttrain_loss: 0.6306,\ttrain_loss_classif: 0.6238,\ttrain_loss_wass: 0.6816,\tval_loss: 0.6187,\tval_loss_classif: 0.6153,\tval_loss_wass: 0.3441\n",
      "6:\t[1s / 7s],\t\ttrain_loss: 0.6226,\ttrain_loss_classif: 0.6188,\ttrain_loss_wass: 0.3745,\tval_loss: 0.6140,\tval_loss_classif: 0.6116,\tval_loss_wass: 0.2489\n",
      "7:\t[0s / 8s],\t\ttrain_loss: 0.6142,\ttrain_loss_classif: 0.6121,\ttrain_loss_wass: 0.2089,\tval_loss: 0.6047,\tval_loss_classif: 0.6028,\tval_loss_wass: 0.1905\n",
      "8:\t[0s / 9s],\t\ttrain_loss: 0.5968,\ttrain_loss_classif: 0.5951,\ttrain_loss_wass: 0.1675,\tval_loss: 0.5900,\tval_loss_classif: 0.5880,\tval_loss_wass: 0.2010\n",
      "9:\t[0s / 9s],\t\ttrain_loss: 0.5825,\ttrain_loss_classif: 0.5807,\ttrain_loss_wass: 0.1745,\tval_loss: 0.5615,\tval_loss_classif: 0.5592,\tval_loss_wass: 0.2339\n",
      "10:\t[0s / 10s],\t\ttrain_loss: 0.5658,\ttrain_loss_classif: 0.5637,\ttrain_loss_wass: 0.2130,\tval_loss: 0.5536,\tval_loss_classif: 0.5517,\tval_loss_wass: 0.1833\n",
      "11:\t[0s / 10s],\t\ttrain_loss: 0.5507,\ttrain_loss_classif: 0.5490,\ttrain_loss_wass: 0.1681,\tval_loss: 0.5099,\tval_loss_classif: 0.5083,\tval_loss_wass: 0.1606\n",
      "12:\t[0s / 11s],\t\ttrain_loss: 0.5190,\ttrain_loss_classif: 0.5172,\ttrain_loss_wass: 0.1798,\tval_loss: 0.5472,\tval_loss_classif: 0.5455,\tval_loss_wass: 0.1640\n",
      "13:\t[0s / 12s],\t\ttrain_loss: 0.5157,\ttrain_loss_classif: 0.5139,\ttrain_loss_wass: 0.1728,\tval_loss: 0.4755,\tval_loss_classif: 0.4735,\tval_loss_wass: 0.1943\n",
      "14:\t[0s / 12s],\t\ttrain_loss: 0.5012,\ttrain_loss_classif: 0.4993,\ttrain_loss_wass: 0.1880,\tval_loss: 0.4601,\tval_loss_classif: 0.4583,\tval_loss_wass: 0.1808\n",
      "15:\t[0s / 13s],\t\ttrain_loss: 0.4616,\ttrain_loss_classif: 0.4597,\ttrain_loss_wass: 0.1922,\tval_loss: 0.4646,\tval_loss_classif: 0.4627,\tval_loss_wass: 0.1920\n",
      "16:\t[0s / 13s],\t\ttrain_loss: 0.4462,\ttrain_loss_classif: 0.4445,\ttrain_loss_wass: 0.1714,\tval_loss: 0.4243,\tval_loss_classif: 0.4224,\tval_loss_wass: 0.1872\n",
      "17:\t[0s / 14s],\t\ttrain_loss: 0.4154,\ttrain_loss_classif: 0.4136,\ttrain_loss_wass: 0.1802,\tval_loss: 0.4705,\tval_loss_classif: 0.4688,\tval_loss_wass: 0.1744\n",
      "18:\t[0s / 14s],\t\ttrain_loss: 0.4197,\ttrain_loss_classif: 0.4181,\ttrain_loss_wass: 0.1569,\tval_loss: 0.3828,\tval_loss_classif: 0.3809,\tval_loss_wass: 0.1909\n",
      "19:\t[0s / 15s],\t\ttrain_loss: 0.3876,\ttrain_loss_classif: 0.3860,\ttrain_loss_wass: 0.1595,\tval_loss: 0.3939,\tval_loss_classif: 0.3919,\tval_loss_wass: 0.1918\n",
      "20:\t[0s / 15s],\t\ttrain_loss: 0.3696,\ttrain_loss_classif: 0.3681,\ttrain_loss_wass: 0.1478,\tval_loss: 0.3595,\tval_loss_classif: 0.3580,\tval_loss_wass: 0.1489\n",
      "21:\t[0s / 16s],\t\ttrain_loss: 0.3651,\ttrain_loss_classif: 0.3636,\ttrain_loss_wass: 0.1587,\tval_loss: 0.3644,\tval_loss_classif: 0.3628,\tval_loss_wass: 0.1536\n",
      "22:\t[0s / 17s],\t\ttrain_loss: 0.3515,\ttrain_loss_classif: 0.3502,\ttrain_loss_wass: 0.1369,\tval_loss: 0.3270,\tval_loss_classif: 0.3256,\tval_loss_wass: 0.1492\n",
      "23:\t[0s / 17s],\t\ttrain_loss: 0.3580,\ttrain_loss_classif: 0.3566,\ttrain_loss_wass: 0.1406,\tval_loss: 0.4111,\tval_loss_classif: 0.4096,\tval_loss_wass: 0.1543\n",
      "24:\t[0s / 18s],\t\ttrain_loss: 0.3760,\ttrain_loss_classif: 0.3743,\ttrain_loss_wass: 0.1657,\tval_loss: 0.3638,\tval_loss_classif: 0.3625,\tval_loss_wass: 0.1366\n",
      "25:\t[0s / 18s],\t\ttrain_loss: 0.3285,\ttrain_loss_classif: 0.3272,\ttrain_loss_wass: 0.1292,\tval_loss: 0.3100,\tval_loss_classif: 0.3085,\tval_loss_wass: 0.1567\n",
      "26:\t[0s / 19s],\t\ttrain_loss: 0.3838,\ttrain_loss_classif: 0.3824,\ttrain_loss_wass: 0.1441,\tval_loss: 0.3614,\tval_loss_classif: 0.3600,\tval_loss_wass: 0.1408\n",
      "27:\t[0s / 20s],\t\ttrain_loss: 0.3163,\ttrain_loss_classif: 0.3150,\ttrain_loss_wass: 0.1316,\tval_loss: 0.2914,\tval_loss_classif: 0.2900,\tval_loss_wass: 0.1371\n",
      "28:\t[0s / 20s],\t\ttrain_loss: 0.3271,\ttrain_loss_classif: 0.3258,\ttrain_loss_wass: 0.1336,\tval_loss: 0.2889,\tval_loss_classif: 0.2875,\tval_loss_wass: 0.1374\n",
      "29:\t[0s / 21s],\t\ttrain_loss: 0.3256,\ttrain_loss_classif: 0.3244,\ttrain_loss_wass: 0.1252,\tval_loss: 0.3430,\tval_loss_classif: 0.3416,\tval_loss_wass: 0.1306\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:12:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.650  0.656  0.836  0.847  0.565  0.247  0.280       0.277   \n",
      "lgbm       0.859  0.853  0.876  0.835  0.150  0.199  0.160       0.274   \n",
      "xgb        0.859  0.853  0.871  0.836  0.150  0.192  0.161       0.271   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.274  \n",
      "lgbm            0.261  \n",
      "xgb             0.264  \n",
      "{'wd_param': 6.0, 'wd': 273.0181579589844, 'y_0_perc': 46, 'y_1_perc': 61}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_6.0_wd_273.0181579589844}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.650 &  0.656 &  0.836 &  0.847 & 0.565 &  0.247 &  0.280 &       0.277 &       0.274 \\\\\n",
      " 0.859 &  0.853 &  0.876 &  0.835 & 0.150 &  0.199 &  0.160 &       0.274 &       0.261 \\\\\n",
      " 0.859 &  0.853 &  0.871 &  0.836 & 0.150 &  0.192 &  0.161 &       0.271 &       0.264 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [02:03<01:01, 61.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD =  258.3604736328125\n",
      "tt = 1 : 50 % \n",
      "Y_0 = 1 : 49 % \n",
      "Y_1 = 1 : 62 % \n",
      "KLD = 0.10057736863301482\n",
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n",
      "Evaluation for model ClassCaus\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6932,\ttrain_loss_classif: 0.6895,\ttrain_loss_wass: 0.3707,\tval_loss: 0.6843,\tval_loss_classif: 0.6819,\tval_loss_wass: 0.2460\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 0.6865,\ttrain_loss_classif: 0.6852,\ttrain_loss_wass: 0.1277,\tval_loss: 0.6799,\tval_loss_classif: 0.6782,\tval_loss_wass: 0.1665\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 0.6822,\ttrain_loss_classif: 0.6815,\ttrain_loss_wass: 0.0765,\tval_loss: 0.6749,\tval_loss_classif: 0.6733,\tval_loss_wass: 0.1575\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 0.6770,\ttrain_loss_classif: 0.6759,\ttrain_loss_wass: 0.1047,\tval_loss: 0.6712,\tval_loss_classif: 0.6693,\tval_loss_wass: 0.1961\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 0.6663,\ttrain_loss_classif: 0.6651,\ttrain_loss_wass: 0.1231,\tval_loss: 0.6658,\tval_loss_classif: 0.6637,\tval_loss_wass: 0.2154\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 0.6565,\ttrain_loss_classif: 0.6546,\ttrain_loss_wass: 0.1899,\tval_loss: 0.6603,\tval_loss_classif: 0.6572,\tval_loss_wass: 0.3116\n",
      "6:\t[1s / 5s],\t\ttrain_loss: 0.6508,\ttrain_loss_classif: 0.6483,\ttrain_loss_wass: 0.2538,\tval_loss: 0.6654,\tval_loss_classif: 0.6618,\tval_loss_wass: 0.3568\n",
      "7:\t[1s / 6s],\t\ttrain_loss: 0.6572,\ttrain_loss_classif: 0.6546,\ttrain_loss_wass: 0.2641,\tval_loss: 0.6583,\tval_loss_classif: 0.6556,\tval_loss_wass: 0.2751\n",
      "8:\t[0s / 7s],\t\ttrain_loss: 0.6452,\ttrain_loss_classif: 0.6440,\ttrain_loss_wass: 0.1238,\tval_loss: 0.6540,\tval_loss_classif: 0.6523,\tval_loss_wass: 0.1769\n",
      "9:\t[0s / 7s],\t\ttrain_loss: 0.6405,\ttrain_loss_classif: 0.6395,\ttrain_loss_wass: 0.0912,\tval_loss: 0.6437,\tval_loss_classif: 0.6420,\tval_loss_wass: 0.1638\n",
      "10:\t[0s / 8s],\t\ttrain_loss: 0.6269,\ttrain_loss_classif: 0.6259,\ttrain_loss_wass: 0.1047,\tval_loss: 0.6376,\tval_loss_classif: 0.6359,\tval_loss_wass: 0.1606\n",
      "11:\t[0s / 9s],\t\ttrain_loss: 0.6047,\ttrain_loss_classif: 0.6037,\ttrain_loss_wass: 0.1085,\tval_loss: 0.6132,\tval_loss_classif: 0.6113,\tval_loss_wass: 0.1857\n",
      "12:\t[0s / 9s],\t\ttrain_loss: 0.5771,\ttrain_loss_classif: 0.5757,\ttrain_loss_wass: 0.1342,\tval_loss: 0.6622,\tval_loss_classif: 0.6599,\tval_loss_wass: 0.2309\n",
      "13:\t[0s / 10s],\t\ttrain_loss: 0.5598,\ttrain_loss_classif: 0.5582,\ttrain_loss_wass: 0.1572,\tval_loss: 0.6137,\tval_loss_classif: 0.6114,\tval_loss_wass: 0.2215\n",
      "14:\t[0s / 11s],\t\ttrain_loss: 0.5466,\ttrain_loss_classif: 0.5454,\ttrain_loss_wass: 0.1226,\tval_loss: 0.6477,\tval_loss_classif: 0.6456,\tval_loss_wass: 0.2069\n",
      "15:\t[0s / 11s],\t\ttrain_loss: 0.5220,\ttrain_loss_classif: 0.5209,\ttrain_loss_wass: 0.1105,\tval_loss: 0.5724,\tval_loss_classif: 0.5706,\tval_loss_wass: 0.1814\n",
      "16:\t[0s / 12s],\t\ttrain_loss: 0.5062,\ttrain_loss_classif: 0.5052,\ttrain_loss_wass: 0.1045,\tval_loss: 0.6194,\tval_loss_classif: 0.6177,\tval_loss_wass: 0.1790\n",
      "17:\t[0s / 12s],\t\ttrain_loss: 0.4624,\ttrain_loss_classif: 0.4614,\ttrain_loss_wass: 0.1051,\tval_loss: 0.5317,\tval_loss_classif: 0.5302,\tval_loss_wass: 0.1581\n",
      "18:\t[0s / 13s],\t\ttrain_loss: 0.4645,\ttrain_loss_classif: 0.4636,\ttrain_loss_wass: 0.0960,\tval_loss: 0.5356,\tval_loss_classif: 0.5340,\tval_loss_wass: 0.1609\n",
      "19:\t[0s / 13s],\t\ttrain_loss: 0.4538,\ttrain_loss_classif: 0.4525,\ttrain_loss_wass: 0.1300,\tval_loss: 0.5175,\tval_loss_classif: 0.5158,\tval_loss_wass: 0.1684\n",
      "20:\t[0s / 14s],\t\ttrain_loss: 0.4178,\ttrain_loss_classif: 0.4163,\ttrain_loss_wass: 0.1514,\tval_loss: 0.5120,\tval_loss_classif: 0.5103,\tval_loss_wass: 0.1636\n",
      "21:\t[0s / 15s],\t\ttrain_loss: 0.4021,\ttrain_loss_classif: 0.4006,\ttrain_loss_wass: 0.1512,\tval_loss: 0.4857,\tval_loss_classif: 0.4840,\tval_loss_wass: 0.1607\n",
      "22:\t[0s / 16s],\t\ttrain_loss: 0.3987,\ttrain_loss_classif: 0.3974,\ttrain_loss_wass: 0.1307,\tval_loss: 0.5069,\tval_loss_classif: 0.5053,\tval_loss_wass: 0.1592\n",
      "23:\t[0s / 16s],\t\ttrain_loss: 0.3911,\ttrain_loss_classif: 0.3899,\ttrain_loss_wass: 0.1226,\tval_loss: 0.4598,\tval_loss_classif: 0.4583,\tval_loss_wass: 0.1495\n",
      "24:\t[0s / 17s],\t\ttrain_loss: 0.3640,\ttrain_loss_classif: 0.3629,\ttrain_loss_wass: 0.1103,\tval_loss: 0.4525,\tval_loss_classif: 0.4510,\tval_loss_wass: 0.1570\n",
      "25:\t[0s / 17s],\t\ttrain_loss: 0.3380,\ttrain_loss_classif: 0.3366,\ttrain_loss_wass: 0.1434,\tval_loss: 0.4358,\tval_loss_classif: 0.4342,\tval_loss_wass: 0.1596\n",
      "26:\t[0s / 18s],\t\ttrain_loss: 0.3373,\ttrain_loss_classif: 0.3358,\ttrain_loss_wass: 0.1519,\tval_loss: 0.4442,\tval_loss_classif: 0.4425,\tval_loss_wass: 0.1674\n",
      "27:\t[0s / 19s],\t\ttrain_loss: 0.3237,\ttrain_loss_classif: 0.3224,\ttrain_loss_wass: 0.1235,\tval_loss: 0.5218,\tval_loss_classif: 0.5201,\tval_loss_wass: 0.1751\n",
      "28:\t[0s / 19s],\t\ttrain_loss: 0.3343,\ttrain_loss_classif: 0.3331,\ttrain_loss_wass: 0.1192,\tval_loss: 0.4512,\tval_loss_classif: 0.4497,\tval_loss_wass: 0.1526\n",
      "29:\t[0s / 20s],\t\ttrain_loss: 0.3100,\ttrain_loss_classif: 0.3087,\ttrain_loss_wass: 0.1276,\tval_loss: 0.4387,\tval_loss_classif: 0.4372,\tval_loss_wass: 0.1494\n",
      "Evaluation for model ClassCaus done\n",
      "Evaluation for model lgbm\n",
      "Evaluation for model lgbm done\n",
      "Evaluation for model xgb\n",
      "[03:13:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Evaluation for model xgb done\n",
      "           acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.656  0.669  0.806  0.778  0.510  0.286  0.289       0.336   \n",
      "lgbm       0.825  0.856  0.821  0.815  0.141  0.192  0.177       0.284   \n",
      "xgb        0.825  0.856  0.842  0.810  0.141  0.185  0.180       0.279   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.340  \n",
      "lgbm            0.285  \n",
      "xgb             0.282  \n",
      "{'wd_param': 6.0, 'wd': 258.3604736328125, 'y_0_perc': 49, 'y_1_perc': 62}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{}\n",
      "\\label{wd_para_6.0_wd_258.3604736328125}\n",
      "\\begin{tabular}{rrrrrrrrr}\n",
      "\\\\hline\n",
      " acc\\_0 &  acc\\_1 &  auc\\_0 &  auc\\_1 &  pehe &  DTV\\_0 &  DTV\\_1 &  std\\_diff\\_0 &  std\\_diff\\_1 \\\\\n",
      "\\\\hline\n",
      " 0.656 &  0.669 &  0.806 &  0.778 & 0.510 &  0.286 &  0.289 &       0.336 &       0.340 \\\\\n",
      " 0.825 &  0.856 &  0.821 &  0.815 & 0.141 &  0.192 &  0.177 &       0.284 &       0.285 \\\\\n",
      " 0.825 &  0.856 &  0.842 &  0.810 & 0.141 &  0.185 &  0.180 &       0.279 &       0.282 \\\\\n",
      "\\\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:08<00:00, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wd_para_0.0_exp_0': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.766  0.778  0.851  0.886  0.117  0.139  0.133       0.178   \n",
      "lgbm       0.812  0.838  0.798  0.818  0.136  0.178  0.130       0.247   \n",
      "xgb        0.812  0.838  0.811  0.823  0.130  0.166  0.129       0.247   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.193  \n",
      "lgbm            0.237  \n",
      "xgb             0.240  , 'wd_param': 0.0, 'exp': 0, 'wd': 24.087810516357422, 'y_0_perc': 47, 'y_1_perc': 63}, 'wd_para_0.0_exp_1': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.819  0.800  0.911  0.888  0.095  0.105  0.076       0.159   \n",
      "lgbm       0.862  0.828  0.848  0.845  0.134  0.176  0.145       0.243   \n",
      "xgb        0.862  0.828  0.856  0.837  0.131  0.176  0.159       0.250   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.145  \n",
      "lgbm            0.233  \n",
      "xgb             0.244  , 'wd_param': 0.0, 'exp': 1, 'wd': 24.286033630371094, 'y_0_perc': 47, 'y_1_perc': 62}, 'wd_para_0.0_exp_2': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.775  0.772  0.863  0.866  0.099  0.138  0.107       0.179   \n",
      "lgbm       0.853  0.828  0.857  0.787  0.115  0.194  0.137       0.255   \n",
      "xgb        0.853  0.828  0.842  0.792  0.128  0.185  0.135       0.257   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.192  \n",
      "lgbm            0.250  \n",
      "xgb             0.252  , 'wd_param': 0.0, 'exp': 2, 'wd': 24.866914749145508, 'y_0_perc': 48, 'y_1_perc': 65}, 'wd_para_2.0_exp_0': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.500  0.619  0.549  0.499  0.157  0.342  0.254       0.362   \n",
      "lgbm       0.794  0.853  0.781  0.780  0.158  0.215  0.181       0.294   \n",
      "xgb        0.794  0.853  0.790  0.798  0.158  0.208  0.176       0.297   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.351  \n",
      "lgbm            0.283  \n",
      "xgb             0.288  , 'wd_param': 2.0, 'exp': 0, 'wd': 76.40813446044922, 'y_0_perc': 49, 'y_1_perc': 61}, 'wd_para_2.0_exp_1': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.519  0.378  0.507  0.500  0.159  0.337  0.246       0.364   \n",
      "lgbm       0.866  0.812  0.836  0.813  0.160  0.193  0.154       0.259   \n",
      "xgb        0.866  0.812  0.826  0.790  0.160  0.209  0.170       0.288   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.345  \n",
      "lgbm            0.249  \n",
      "xgb             0.279  , 'wd_param': 2.0, 'exp': 1, 'wd': 75.38691711425781, 'y_0_perc': 48, 'y_1_perc': 63}, 'wd_para_2.0_exp_2': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.781  0.834  0.866  0.923  0.138  0.124  0.123       0.202   \n",
      "lgbm       0.831  0.856  0.828  0.840  0.158  0.179  0.128       0.237   \n",
      "xgb        0.831  0.856  0.815  0.845  0.158  0.177  0.145       0.252   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.197  \n",
      "lgbm            0.224  \n",
      "xgb             0.245  , 'wd_param': 2.0, 'exp': 2, 'wd': 65.29910278320312, 'y_0_perc': 51, 'y_1_perc': 63}, 'wd_para_4.0_exp_0': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.612  0.778  0.856  0.855  0.429  0.255  0.173       0.299   \n",
      "lgbm       0.844  0.878  0.850  0.867  0.148  0.195  0.173       0.273   \n",
      "xgb        0.844  0.878  0.843  0.869  0.148  0.193  0.160       0.284   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.247  \n",
      "lgbm            0.269  \n",
      "xgb             0.277  , 'wd_param': 4.0, 'exp': 0, 'wd': 126.32049560546875, 'y_0_perc': 48, 'y_1_perc': 60}, 'wd_para_4.0_exp_1': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.738  0.666  0.817  0.792  0.381  0.187  0.281       0.256   \n",
      "lgbm       0.828  0.822  0.799  0.833  0.156  0.186  0.182       0.261   \n",
      "xgb        0.828  0.822  0.808  0.835  0.156  0.182  0.194       0.265   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.287  \n",
      "lgbm            0.259  \n",
      "xgb             0.265  , 'wd_param': 4.0, 'exp': 1, 'wd': 278.48052978515625, 'y_0_perc': 49, 'y_1_perc': 62}, 'wd_para_4.0_exp_2': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.666  0.719  0.828  0.784  0.394  0.241  0.279       0.290   \n",
      "lgbm       0.853  0.812  0.761  0.767  0.163  0.224  0.187       0.294   \n",
      "xgb        0.853  0.812  0.764  0.771  0.163  0.221  0.202       0.295   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.305  \n",
      "lgbm            0.294  \n",
      "xgb             0.298  , 'wd_param': 4.0, 'exp': 2, 'wd': 159.47384643554688, 'y_0_perc': 47, 'y_1_perc': 60}, 'wd_para_6.0_exp_0': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.638  0.656  0.784  0.783  0.553  0.270  0.281       0.343   \n",
      "lgbm       0.838  0.847  0.862  0.865  0.140  0.163  0.157       0.255   \n",
      "xgb        0.838  0.847  0.862  0.868  0.140  0.156  0.167       0.250   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.322  \n",
      "lgbm            0.254  \n",
      "xgb             0.252  , 'wd_param': 6.0, 'exp': 0, 'wd': 708.8859252929688, 'y_0_perc': 48, 'y_1_perc': 61}, 'wd_para_6.0_exp_1': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.650  0.656  0.836  0.847  0.565  0.247  0.280       0.277   \n",
      "lgbm       0.859  0.853  0.876  0.835  0.150  0.199  0.160       0.274   \n",
      "xgb        0.859  0.853  0.871  0.836  0.150  0.192  0.161       0.271   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.274  \n",
      "lgbm            0.261  \n",
      "xgb             0.264  , 'wd_param': 6.0, 'exp': 1, 'wd': 273.0181579589844, 'y_0_perc': 46, 'y_1_perc': 61}, 'wd_para_6.0_exp_2': {'df':            acc_0  acc_1  auc_0  auc_1   pehe  DTV_0  DTV_1  std_diff_0  \\\n",
      "ClassCaus  0.656  0.669  0.806  0.778  0.510  0.286  0.289       0.336   \n",
      "lgbm       0.825  0.856  0.821  0.815  0.141  0.192  0.177       0.284   \n",
      "xgb        0.825  0.856  0.842  0.810  0.141  0.185  0.180       0.279   \n",
      "\n",
      "           std_diff_1  \n",
      "ClassCaus       0.340  \n",
      "lgbm            0.285  \n",
      "xgb             0.282  , 'wd_param': 6.0, 'exp': 2, 'wd': 258.3604736328125, 'y_0_perc': 49, 'y_1_perc': 62}}\n",
      "['\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.766 &  0.778 &  0.851 &  0.886 & 0.117 &  0.139 &  0.133 &       0.178 &       0.193 \\\\\\\\\\n 0.812 &  0.838 &  0.798 &  0.818 & 0.136 &  0.178 &  0.130 &       0.247 &       0.237 \\\\\\\\\\n 0.812 &  0.838 &  0.811 &  0.823 & 0.130 &  0.166 &  0.129 &       0.247 &       0.240 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.819 &  0.800 &  0.911 &  0.888 & 0.095 &  0.105 &  0.076 &       0.159 &       0.145 \\\\\\\\\\n 0.862 &  0.828 &  0.848 &  0.845 & 0.134 &  0.176 &  0.145 &       0.243 &       0.233 \\\\\\\\\\n 0.862 &  0.828 &  0.856 &  0.837 & 0.131 &  0.176 &  0.159 &       0.250 &       0.244 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.775 &  0.772 &  0.863 &  0.866 & 0.099 &  0.138 &  0.107 &       0.179 &       0.192 \\\\\\\\\\n 0.853 &  0.828 &  0.857 &  0.787 & 0.115 &  0.194 &  0.137 &       0.255 &       0.250 \\\\\\\\\\n 0.853 &  0.828 &  0.842 &  0.792 & 0.128 &  0.185 &  0.135 &       0.257 &       0.252 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.500 &  0.619 &  0.549 &  0.499 & 0.157 &  0.342 &  0.254 &       0.362 &       0.351 \\\\\\\\\\n 0.794 &  0.853 &  0.781 &  0.780 & 0.158 &  0.215 &  0.181 &       0.294 &       0.283 \\\\\\\\\\n 0.794 &  0.853 &  0.790 &  0.798 & 0.158 &  0.208 &  0.176 &       0.297 &       0.288 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.519 &  0.378 &  0.507 &  0.500 & 0.159 &  0.337 &  0.246 &       0.364 &       0.345 \\\\\\\\\\n 0.866 &  0.812 &  0.836 &  0.813 & 0.160 &  0.193 &  0.154 &       0.259 &       0.249 \\\\\\\\\\n 0.866 &  0.812 &  0.826 &  0.790 & 0.160 &  0.209 &  0.170 &       0.288 &       0.279 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.781 &  0.834 &  0.866 &  0.923 & 0.138 &  0.124 &  0.123 &       0.202 &       0.197 \\\\\\\\\\n 0.831 &  0.856 &  0.828 &  0.840 & 0.158 &  0.179 &  0.128 &       0.237 &       0.224 \\\\\\\\\\n 0.831 &  0.856 &  0.815 &  0.845 & 0.158 &  0.177 &  0.145 &       0.252 &       0.245 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.612 &  0.778 &  0.856 &  0.855 & 0.429 &  0.255 &  0.173 &       0.299 &       0.247 \\\\\\\\\\n 0.844 &  0.878 &  0.850 &  0.867 & 0.148 &  0.195 &  0.173 &       0.273 &       0.269 \\\\\\\\\\n 0.844 &  0.878 &  0.843 &  0.869 & 0.148 &  0.193 &  0.160 &       0.284 &       0.277 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.738 &  0.666 &  0.817 &  0.792 & 0.381 &  0.187 &  0.281 &       0.256 &       0.287 \\\\\\\\\\n 0.828 &  0.822 &  0.799 &  0.833 & 0.156 &  0.186 &  0.182 &       0.261 &       0.259 \\\\\\\\\\n 0.828 &  0.822 &  0.808 &  0.835 & 0.156 &  0.182 &  0.194 &       0.265 &       0.265 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.666 &  0.719 &  0.828 &  0.784 & 0.394 &  0.241 &  0.279 &       0.290 &       0.305 \\\\\\\\\\n 0.853 &  0.812 &  0.761 &  0.767 & 0.163 &  0.224 &  0.187 &       0.294 &       0.294 \\\\\\\\\\n 0.853 &  0.812 &  0.764 &  0.771 & 0.163 &  0.221 &  0.202 &       0.295 &       0.298 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.638 &  0.656 &  0.784 &  0.783 & 0.553 &  0.270 &  0.281 &       0.343 &       0.322 \\\\\\\\\\n 0.838 &  0.847 &  0.862 &  0.865 & 0.140 &  0.163 &  0.157 &       0.255 &       0.254 \\\\\\\\\\n 0.838 &  0.847 &  0.862 &  0.868 & 0.140 &  0.156 &  0.167 &       0.250 &       0.252 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.650 &  0.656 &  0.836 &  0.847 & 0.565 &  0.247 &  0.280 &       0.277 &       0.274 \\\\\\\\\\n 0.859 &  0.853 &  0.876 &  0.835 & 0.150 &  0.199 &  0.160 &       0.274 &       0.261 \\\\\\\\\\n 0.859 &  0.853 &  0.871 &  0.836 & 0.150 &  0.192 &  0.161 &       0.271 &       0.264 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n', '\\\\begin{tabular}{rrrrrrrrr}\\n\\\\toprule\\n acc\\\\_0 &  acc\\\\_1 &  auc\\\\_0 &  auc\\\\_1 &  pehe &  DTV\\\\_0 &  DTV\\\\_1 &  std\\\\_diff\\\\_0 &  std\\\\_diff\\\\_1 \\\\\\\\\\n\\\\midrule\\n 0.656 &  0.669 &  0.806 &  0.778 & 0.510 &  0.286 &  0.289 &       0.336 &       0.340 \\\\\\\\\\n 0.825 &  0.856 &  0.821 &  0.815 & 0.141 &  0.192 &  0.177 &       0.284 &       0.285 \\\\\\\\\\n 0.825 &  0.856 &  0.842 &  0.810 & 0.141 &  0.185 &  0.180 &       0.279 &       0.282 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tabletex import table_tex\n",
    "from matplotlib.pyplot import cla, xlabel\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "from classes import *\n",
    "from simulation import *\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import sys\n",
    "# sys.path.append('./')\n",
    "#import livejson\n",
    "import json\n",
    "#import SessionState\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "param_sim = {\n",
    "    'n_features': 25,\n",
    "    'n_classes': 2,\n",
    "    'n_samples': 1000,\n",
    "    'wd_para': 2.,\n",
    "    'beta': [0.1, 0.1, 0.3],\n",
    "    'coef_tt': 1.1,\n",
    "    'rho': 0.1,\n",
    "    'path_data': './dataclassif.csv'\n",
    "}\n",
    "\n",
    "params_classifcaus = {\n",
    "    \"encoded_features\": 25,\n",
    "    \"alpha_wass\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 0.001,\n",
    "    \"patience\": 7,\n",
    "}\n",
    "\n",
    "idx = np.arange(param_sim['n_features'])\n",
    "param_sim['beta'] = (-1) ** idx * np.exp(-idx / 20.)\n",
    "\n",
    "# class for Experiment: for evry wd_para simulate new data and luanch benchmark\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, param_sim, params_classifcaus, list_wd_param, num_exp, list_models, mode=\"onemodel\"):\n",
    "        self.param_sim = param_sim\n",
    "        self.params_classifcaus = params_classifcaus\n",
    "        self.list_wd_param = list_wd_param\n",
    "        self.num_exp = num_exp\n",
    "        self.list_models = list_models\n",
    "        self.mode = mode\n",
    "        self.d_all_exp = {}\n",
    "\n",
    "    def save_results_exp(self, i_exp):\n",
    "        d = {'i_exp': i_exp, 'list_wd_param': self.list_wd_param, 'list_models': self.list_models, 'mode': self.mode,\n",
    "             'num_exp': self.num_exp, 'param_sim': self.param_sim, 'params_classifcaus': self.params_classifcaus}\n",
    "        self.d_all_exp[i_exp] = d\n",
    "\n",
    "    def run(self):\n",
    "        d = {}\n",
    "        d_latex = []\n",
    "        for wd_para in self.list_wd_param:\n",
    "            d_sim = {}\n",
    "            for exp in tqdm(range(self.num_exp)):\n",
    "                #print(f'wd_para: {wd_para}, exp: {exp}')\n",
    "                # print('Simulation')\n",
    "                self.param_sim['wd_para'] = wd_para\n",
    "                self.sim = Simulation(self.param_sim)\n",
    "                self.sim.simule()\n",
    "                # get param\n",
    "                d_sim['wd_para'] = wd_para\n",
    "                d_sim['wd'] = self.sim.wd\n",
    "                d_sim['y_0_perc'] = self.sim.y_0_perc\n",
    "                d_sim['y_1_perc'] = self.sim.y_1_perc\n",
    "\n",
    "                # Bnechmark\n",
    "                # print('Benchmark')\n",
    "                Bench = BenchmarkClassif(\n",
    "                    self.params_classifcaus, self.list_models)\n",
    "\n",
    "                df_results, dic_fig, d_all = Bench.evall_all_bench(\n",
    "                    mode=self.mode)\n",
    "\n",
    "                #print('End of Benchmark')\n",
    "                df_results = df_results.drop(['f1_0', 'f1_1', 'mae'], axis=1)\n",
    "                # rename columns\n",
    "                df_results.columns.values[-3] = 'DTV_1'\n",
    "                df_results.columns.values[-4] = 'DTV_0'\n",
    "                # round values to 3 decimals\n",
    "                df_results = df_results.round(3)\n",
    "                print(df_results)\n",
    "                d[f'wd_para_{wd_para}_exp_{exp}'] = {'df': df_results, 'wd_param': wd_para, 'exp': exp,\n",
    "                                                     'wd': self.sim.wd, 'y_0_perc': self.sim.y_0_perc, 'y_1_perc': self.sim.y_1_perc}\n",
    "\n",
    "                print({'wd_param': wd_para,\n",
    "                       'wd': self.sim.wd, 'y_0_perc': self.sim.y_0_perc, 'y_1_perc': self.sim.y_1_perc})\n",
    "                # display results\n",
    "                # print(df_results)\n",
    "                # save results as csv\n",
    "                # to latex \n",
    "                d_latex.append(table_tex(df = df_results, title=f'wd_para_{wd_para}_wd_{self.sim.wd}') )\n",
    "                                         \n",
    "                fig_0 = boxplot_F(\n",
    "                    list_models, d_all, ylabel='diff proba 0', option=0)\n",
    "                plt.savefig(f\"boxplot_F_{exp}_{mode}.png\")\n",
    "                fig_1 = boxplot_F(\n",
    "                    list_models, d_all, ylabel='diff proba 1', option=1)\n",
    "                plt.savefig(f\"boxplot_F_{exp}_{mode}.png\")\n",
    "                plt.close('all')\n",
    "\n",
    "                fig_dist_0_list = dist_F(\n",
    "                    list_models, d_all, ylabel='', option=0)\n",
    "                fig_dist_1_list = dist_F(\n",
    "                    list_models, d_all, ylabel='', option=1)\n",
    "                plt.close('all')\n",
    "                self.save_results_exp(exp)\n",
    "\n",
    "        ##print(f'd_sim: {d_sim}')\n",
    "        ##print(f'd_all_exp: {self.d_all_exp}')\n",
    "        # print(d)\n",
    "        self.d = d\n",
    "        self.d_latex = d_latex\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    list_models = [\"ClassCaus\", \"lgbm\", \"xgb\"]\n",
    "\n",
    "    num_exp = 3\n",
    "    mode = \"onemodel\"\n",
    "    # get list of wd_param\n",
    "    list_wd_param = [0., 2., 4., 6.]\n",
    "    #np.arange(0.1, 5., 0.5)\n",
    "    # create object\n",
    "    exp = Experiment(param_sim, params_classifcaus,\n",
    "                     list_wd_param, num_exp, list_models, mode=mode)\n",
    "    # run\n",
    "    exp.run()\n",
    "    print(exp.d)\n",
    "    print(exp.d_latex)\n",
    "    # save as pickle\n",
    "    \"\"\"with open('results_exp.pkl', 'wb') as fp:\n",
    "        pkl.dump(exp.d, fp)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11c6ae450426ff67e02db51d48536f215792ac6d7e270c76f6c7b3ccd10304c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
